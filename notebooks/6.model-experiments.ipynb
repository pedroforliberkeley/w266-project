{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205a9e54",
   "metadata": {},
   "source": [
    "# Balancing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5e36e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4782f5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54e281e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:24.835532Z",
     "start_time": "2023-07-30T12:14:18.174280Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import torch\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c412bec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:24.850973Z",
     "start_time": "2023-07-30T12:14:24.836974Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17b6bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:24.866982Z",
     "start_time": "2023-07-30T12:14:24.852976Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96db02cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:24.914870Z",
     "start_time": "2023-07-30T12:14:24.868973Z"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682e6e9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:24.930892Z",
     "start_time": "2023-07-30T12:14:24.915863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0aca1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e55a9e",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c392e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:25.088892Z",
     "start_time": "2023-07-30T12:14:24.931901Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(\"../data/clean_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5a9f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71775a39",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762431da",
   "metadata": {},
   "source": [
    "### Balancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037e2ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:26.433780Z",
     "start_time": "2023-07-30T12:14:25.089891Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "\n",
    "\n",
    "SR = random.SystemRandom()\n",
    "LANGUAGES = [\"es\", \"de\", \"fr\", \"ar\", \"te\", \"hi\", \"ja\", \"fa\", \"sq\", \"bg\", \"nl\", \"gu\", \"ig\", \"kk\", \"mt\", \"ps\"]\n",
    "TRANSLATIONS = dict()\n",
    "\n",
    "\n",
    "def data_augmentation(message: str, language: str = \"en\", aug_range: int = 1) -> list:\n",
    "    \"\"\"\n",
    "    Create new text data by translating a message to a random language\n",
    "    and then tranlating it back\n",
    "    \n",
    "    :param message: messege to be translated\n",
    "    :param language: original language of the message\n",
    "    :param aug_range: number of new messages to generate\n",
    "    :return: list of new messages\n",
    "    \"\"\"\n",
    "    augmented_messages = []\n",
    "    if hasattr(message, \"decode\"):\n",
    "        message = message.decode(\"utf-8\")\n",
    "\n",
    "    for j in range(0, aug_range) :\n",
    "        text_blob = TextBlob(message)\n",
    "        try:\n",
    "            to_lang = SR.choice(LANGUAGES)\n",
    "            \n",
    "            if (message, to_lang) in TRANSLATIONS:\n",
    "                text_blob = TRANSLATIONS[(message, to_lang)]\n",
    "            else:\n",
    "                text_blob = text_blob.translate(from_lang=language, to=to_lang)\n",
    "                text_blob = text_blob.translate(from_lang=to_lang, to=language)\n",
    "                TRANSLATIONS[(message, to_lang)] = str(text_blob)\n",
    "        except NotTranslated:\n",
    "            pass\n",
    "        else:\n",
    "            augmented_messages.append(TRANSLATIONS[(message, to_lang)])\n",
    "\n",
    "    return augmented_messages\n",
    "\n",
    "\n",
    "def apply_balancing(data: pd.DataFrame, target: str, augmented: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply balancing to the dataset based on the selected strategy\n",
    "    \n",
    "    :param data: dataset to be balanced\n",
    "    :param target: target strategy (avg or max)\n",
    "    :param augmented: flag if we should apply augmentation (back-and-forth translation)\n",
    "    :return: balanced dataframe\n",
    "    \"\"\"\n",
    "    assert (augmented and target == \"avg\") or not augmented\n",
    "    \n",
    "    # get the file name with the augmented data\n",
    "    if augmented:\n",
    "        file_name = f\"balanced_augmented_{target}.parquet\"\n",
    "    else:\n",
    "        file_name = f\"balanced_{target}.parquet\"\n",
    "    \n",
    "    # if the file exists load and returns it\n",
    "    if os.path.exists(f\"../data/{file_name}\"):\n",
    "        return pd.read_parquet(f\"../data/{file_name}\")\n",
    "    \n",
    "    # select the training data\n",
    "    train = data.loc[lambda f: f[\"set\"] == \"train\"].copy()\n",
    "    train[\"augmented\"] = False\n",
    "    train[\"for_balance\"] = False\n",
    "    \n",
    "    # calculate the representation of each class\n",
    "    class_representation = train.goemotion.value_counts()\n",
    "    minority = class_representation.min()\n",
    "    majority = class_representation.max()\n",
    "    avg = int(class_representation.mean())\n",
    "\n",
    "    # choose what is the target amount of each class based on the strategy\n",
    "    if target == \"max\":\n",
    "        target_value = majority\n",
    "    elif target == \"avg\":\n",
    "        target_value = avg\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    # for each emotion of interest\n",
    "    final = list()\n",
    "    for emotion in class_representation.index:\n",
    "        # get the data with that emotion\n",
    "        edata = train.loc[lambda f: f[\"goemotion\"] == emotion]\n",
    "        representation = class_representation.loc[emotion]\n",
    "        to_generate = target_value - representation\n",
    "        \n",
    "        # if we have less than the target value, we should balance it\n",
    "        if representation < target_value:\n",
    "            # if we don't want any augmentation, do a simple sampling of the data\n",
    "            if not augmented:\n",
    "                sampled = pd.concat([edata, edata.sample(to_generate, replace=True)])\n",
    "            \n",
    "            # otherwise\n",
    "            else:\n",
    "                # sample some text from the original dataset\n",
    "                generated = list()\n",
    "                sampled = edata.sample(to_generate, replace=(to_generate > representation)).reset_index(drop=True)\n",
    "                \n",
    "                # for each text in the sample\n",
    "                for row in tqdm(sampled.itertuples(name=None), total=sampled.shape[0]):\n",
    "                    # try to perform the translation of text at least 3 times\n",
    "                    for i in range(3):\n",
    "                        try:\n",
    "                            sampled.loc[row[0], \"text\"] = data_augmentation(row[-4])[0]\n",
    "                            sampled.loc[row[0], \"augmented\"] = True\n",
    "                        except IndexError:\n",
    "                            continue\n",
    "                        except urllib.error.URLError:\n",
    "                            time.sleep(3)\n",
    "                        else:\n",
    "                            break\n",
    "                \n",
    "                # save the final dataset\n",
    "                sampled[\"for_balance\"] = True\n",
    "                sampled = pd.concat([edata, sampled])\n",
    "        else:\n",
    "            sampled = edata\n",
    "        \n",
    "        # save the sampled results to the final dataset\n",
    "        final.append(sampled)\n",
    "\n",
    "    # concatenate with the original test set\n",
    "    balanced = pd.concat(final + [data.loc[lambda f: f[\"set\"] != \"train\"].assign(augmented=False, for_balance=False)])\n",
    "    balanced[\"text\"] = balanced[\"text\"].str.replace(\"\\[name\\]\", \"[NAME]\").str.replace(\"\\[religion\\]\", \"[RELIGION]\")\n",
    "    \n",
    "    # export the final result\n",
    "    balanced.to_parquet(f\"../data/{file_name}\")\n",
    "    \n",
    "    \n",
    "    # return the sampled dataset\n",
    "    return balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a80497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:26.526163Z",
     "start_time": "2023-07-30T12:14:26.434780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>goemotion</th>\n",
       "      <th>ekman</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>set</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>augmented</th>\n",
       "      <th>for_balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>edpom9u</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>train</td>\n",
       "      <td>The good kind</td>\n",
       "      <td>good kind</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>ef55x08</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>train</td>\n",
       "      <td>GG. We played as best as we could. Utah just a...</td>\n",
       "      <td>gg played best could utah better team</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>edxzrwk</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>train</td>\n",
       "      <td>[NAME] looking like [NAME] out there</td>\n",
       "      <td>[NAME] looking like [NAME]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>ef9yhnp</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>train</td>\n",
       "      <td>&gt;Allow insurance to not pay for treatment of d...</td>\n",
       "      <td>allow insurance pay treatment disease known va...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>ees5bt3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>train</td>\n",
       "      <td>Shadow of Mordor 3 looks pretty lit</td>\n",
       "      <td>shadow mordor 3 look pretty lit</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63807</th>\n",
       "      <td>eezc65u</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>test</td>\n",
       "      <td>The essay is optional.</td>\n",
       "      <td>essay optional</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63808</th>\n",
       "      <td>edduyro</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>test</td>\n",
       "      <td>Waiting for both of these things is torture</td>\n",
       "      <td>waiting thing torture</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63809</th>\n",
       "      <td>edy4kl7</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>test</td>\n",
       "      <td>Easy just include [NAME] to continue to tormen...</td>\n",
       "      <td>easy include [NAME] continue torment [NAME]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63810</th>\n",
       "      <td>efbiugo</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>test</td>\n",
       "      <td>Daddy issues [NAME]</td>\n",
       "      <td>daddy issue [NAME]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63811</th>\n",
       "      <td>edtjpv6</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>test</td>\n",
       "      <td>Had to watch \"Elmo in Grouchland\" one time too...</td>\n",
       "      <td>watch elmo grouchland one time many kid little...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82431 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          code goemotion    ekman sentiment    set  \\\n",
       "72     edpom9u   neutral  neutral   neutral  train   \n",
       "84     ef55x08   neutral  neutral   neutral  train   \n",
       "106    edxzrwk   neutral  neutral   neutral  train   \n",
       "108    ef9yhnp   neutral  neutral   neutral  train   \n",
       "208    ees5bt3   neutral  neutral   neutral  train   \n",
       "...        ...       ...      ...       ...    ...   \n",
       "63807  eezc65u   neutral  neutral   neutral   test   \n",
       "63808  edduyro   neutral  neutral   neutral   test   \n",
       "63809  edy4kl7   neutral  neutral   neutral   test   \n",
       "63810  efbiugo   neutral  neutral   neutral   test   \n",
       "63811  edtjpv6   neutral  neutral   neutral   test   \n",
       "\n",
       "                                                    text  \\\n",
       "72                                         The good kind   \n",
       "84     GG. We played as best as we could. Utah just a...   \n",
       "106                 [NAME] looking like [NAME] out there   \n",
       "108    >Allow insurance to not pay for treatment of d...   \n",
       "208                  Shadow of Mordor 3 looks pretty lit   \n",
       "...                                                  ...   \n",
       "63807                             The essay is optional.   \n",
       "63808        Waiting for both of these things is torture   \n",
       "63809  Easy just include [NAME] to continue to tormen...   \n",
       "63810                                Daddy issues [NAME]   \n",
       "63811  Had to watch \"Elmo in Grouchland\" one time too...   \n",
       "\n",
       "                                              clean_text  augmented  \\\n",
       "72                                             good kind      False   \n",
       "84                 gg played best could utah better team      False   \n",
       "106                           [NAME] looking like [NAME]      False   \n",
       "108    allow insurance pay treatment disease known va...      False   \n",
       "208                      shadow mordor 3 look pretty lit      False   \n",
       "...                                                  ...        ...   \n",
       "63807                                     essay optional      False   \n",
       "63808                              waiting thing torture      False   \n",
       "63809        easy include [NAME] continue torment [NAME]      False   \n",
       "63810                                 daddy issue [NAME]      False   \n",
       "63811  watch elmo grouchland one time many kid little...      False   \n",
       "\n",
       "       for_balance  \n",
       "72           False  \n",
       "84           False  \n",
       "106          False  \n",
       "108          False  \n",
       "208          False  \n",
       "...            ...  \n",
       "63807        False  \n",
       "63808        False  \n",
       "63809        False  \n",
       "63810        False  \n",
       "63811        False  \n",
       "\n",
       "[82431 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_balancing(dataset, \"avg\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7326d",
   "metadata": {},
   "source": [
    "### Clean Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7209145a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:26.574163Z",
     "start_time": "2023-07-30T12:14:26.528135Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import contractions\n",
    "import emoji\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "TOKENIZER = TweetTokenizer()\n",
    "PUNCUATION_LIST = list(string.punctuation)\n",
    "\n",
    "\n",
    "def reinsert_tags(text: str, tag_split: str, tag_prefix: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Given a certain text, look for a separator that has been used for tag splitting\n",
    "    and make sure that all tags are put between []\n",
    "    \n",
    "    :param text: text to be adjusted\n",
    "    :param tag_split: tag to be splitted\n",
    "    :param tag_prefix: prefix to apply on tagging\n",
    "    :return: tagged text\n",
    "    \"\"\"\n",
    "    open_tag = False\n",
    "    full_sentence = \"\"\n",
    "    for sentence in text.split(tag_split):\n",
    "        if open_tag:\n",
    "            full_sentence += f\" [{tag_prefix}{sentence.upper().strip()}] \"\n",
    "        else:\n",
    "            full_sentence += sentence\n",
    "        open_tag = not open_tag\n",
    "\n",
    "    full_sentence = re.sub(\" +\", \" \", full_sentence)\n",
    "    return full_sentence.strip()\n",
    "\n",
    "\n",
    "def clean_content(\n",
    "    text, \n",
    "    fix_contraction: bool = True, \n",
    "    tag_emoji: bool = True,\n",
    "    tagged_items: list = [\"name\", \"religion\"],\n",
    "    handles: bool = True, \n",
    "    case: bool = True,\n",
    "    links: bool = True,\n",
    "    non_char: bool = True,\n",
    "    rm_stop_words: bool = True,\n",
    "    lemmatization: bool = True,\n",
    "    tokenize: bool = True,\n",
    "    ponctuation: bool = True,\n",
    "    unmapped_emoji: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Apply data cleaning to text given a range of options\n",
    "    \n",
    "    :param fix_contraction: True if we want to remove abbreviations\n",
    "    :param tag_emoji: True if we want to tag emojis by name\n",
    "    :param tagged_items: List of items that are tagged in the current text\n",
    "    :param handles: True if we want to remove twitter like handles\n",
    "    :param case: True if we want to normalize to lower case\n",
    "    :param links: True if we want to remove websites and links\n",
    "    :param non_char: True if we want to remove non-character words\n",
    "    :param rm_stop_words: True if we want to remove stop words\n",
    "    :param lemmatization: True if we want to apply lemmatization\n",
    "    :param tokenize: True if we want to apply twitter tokenization\n",
    "    :param ponctuation: True if we want to remove ponctuation\n",
    "    :param unmapped_emoji: True if we want to remove unmapped emojis\n",
    "    :return: clean text\n",
    "    \"\"\"\n",
    "    clean_text = text\n",
    "    \n",
    "    # replaces abbreviations with full word versions\n",
    "    if fix_contraction:\n",
    "        clean_text = contractions.fix(text)\n",
    "    \n",
    "    # replaces emojis\n",
    "    if tag_emoji:\n",
    "        clean_text = \"\".join(\n",
    "            [c if c not in emoji.EMOJI_DATA else emoji.EMOJI_DATA[c][\"en\"].replace(\":\", \" _emoji_ \") for c in clean_text]\n",
    "        )\n",
    "    \n",
    "    # remove reddit handles\n",
    "    if handles:\n",
    "        clean_text = re.sub(r\"@\\w+\\s?\", \"\", clean_text)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    if case:\n",
    "        clean_text = clean_text.lower()\n",
    "    \n",
    "\n",
    "    if links:\n",
    "         # remove links http:// or https://\n",
    "        clean_text = re.sub(r\"https?:\\/\\/\\S+\", \"\", clean_text)\n",
    "    \n",
    "        # remove links beginning with www. and ending with .com\n",
    "        clean_text = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", \"\", clean_text)\n",
    "    \n",
    "        # remove html reference characters\n",
    "        clean_text = re.sub(r\"&[a-z]+;\", \"\", clean_text)\n",
    "        \n",
    "    # deal with tagged items\n",
    "    for tag in tagged_items:\n",
    "        clean_text = re.sub(fr\"\\[{tag}\\]\", f\" _tag_ {tag} _tag_ \", clean_text)  \n",
    "    \n",
    "    # remove non-letter characters besides spaces \"/\", \";\" \"[\", \"]\" \"=\", \"#\"\n",
    "    if non_char:\n",
    "        clean_text = re.sub(r\"[/;\\[\\]=#]\", \"\", clean_text)  \n",
    "    \n",
    "    # remove stop words\n",
    "    if rm_stop_words:\n",
    "        clean_lst = []\n",
    "        for word in clean_text.split():\n",
    "            if word not in STOP_WORDS:\n",
    "                clean_lst.append(word)\n",
    "    else:\n",
    "        clean_lst = clean_text.split()\n",
    "        \n",
    "    \n",
    "    # apply lemmatization\n",
    "    if lemmatization:\n",
    "        lemmatized_words = []\n",
    "        for word in clean_lst:\n",
    "            lemmatized_word = WordNetLemmatizer().lemmatize(word)\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "        clean_lst = lemmatized_words\n",
    "    \n",
    "    # concatenate the text again\n",
    "    clean_text = \" \".join(clean_lst)\n",
    "    \n",
    "    # apply tokenization\n",
    "    if tokenize:\n",
    "        tokens = TOKENIZER.tokenize(clean_text)\n",
    "    else:\n",
    "        tokens = clean_text.split(\" \")\n",
    "    \n",
    "    if ponctuation:\n",
    "        clean_text = \" \".join([w for w in tokens if w not in PUNCUATION_LIST])\n",
    "    else:\n",
    "        clean_text = \" \".join(tokens)\n",
    "        \n",
    "    # clean emojis that were not mapped by the library\n",
    "    if unmapped_emoji:\n",
    "        clean_text = \"\".join([w for w in clean_text if ord(w) < 2000])\n",
    "        \n",
    "    # add the tags for emojis\n",
    "    if tag_emoji:\n",
    "        clean_text = reinsert_tags(clean_text, \"_emoji_\", \"EMOJI_\")\n",
    "    \n",
    "    # re-insert tags\n",
    "    if len(tagged_items) > 0:\n",
    "        clean_text = reinsert_tags(clean_text, \"_tag_\")\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3a86fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:27.650585Z",
     "start_time": "2023-07-30T12:14:26.576585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regret unfollowing him...\n",
      "regret unfollowing him ...\n"
     ]
    }
   ],
   "source": [
    "text = dataset.text.sample(1).values[0]\n",
    "print(text)\n",
    "print(clean_content(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1515073d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:27.666584Z",
     "start_time": "2023-07-30T12:14:27.652585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no least suspect [EMOJI_THINKING_FACE]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_content(\"No. Or at least thatâ€™s what I suspectðŸ¤”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b05c3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:27.682586Z",
     "start_time": "2023-07-30T12:14:27.667587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No. Or at least thats what I suspect [EMOJI_THINKING_FACE]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_content(\n",
    "    \"No. Or at least thatâ€™s what I suspectðŸ¤”\", \n",
    "    fix_contraction=False, \n",
    "    tag_emoji=True,\n",
    "    tagged_items=[\"NAME\", \"RELIGION\"],\n",
    "    handles=False, \n",
    "    case=False,\n",
    "    links=False,\n",
    "    non_char=False,\n",
    "    rm_stop_words=False,\n",
    "    lemmatization=False,\n",
    "    tokenize=False,\n",
    "    ponctuation=False,\n",
    "    unmapped_emoji=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea0056",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8608bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:27.947585Z",
     "start_time": "2023-07-30T12:14:27.683587Z"
    }
   },
   "outputs": [],
   "source": [
    "EMOJIS_FOUND = (\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            (c,) \n",
    "            for text in dataset[\"text\"].to_list()\n",
    "            for c in text\n",
    "            if c in emoji.EMOJI_DATA \n",
    "        ],\n",
    "        columns=[\"emoji\"]\n",
    "    )\n",
    "    .assign(count=1)\n",
    "    .groupby([\"emoji\"], as_index=False)[\"count\"]\n",
    "    .sum()\n",
    "    .assign(pct=lambda f: f[\"count\"] / f[\"count\"].sum())\n",
    "    .assign(cum_pct=lambda f: f[\"pct\"].cumsum())\n",
    ")\n",
    "\n",
    "\n",
    "def apply_tokenization(\n",
    "    t_model: TFAutoModel, \n",
    "    tokenizer: AutoTokenizer, \n",
    "    data: pd.DataFrame, \n",
    "    emoji_tagging: bool = False, \n",
    "    emoji_threshold: float = 0.8\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Given a tokenizer object and some data, apply the tokenization process on the\n",
    "    train, test and validation sets and generate a dictionary of processed data with\n",
    "    all inputs needed for the model\n",
    "    \n",
    "    :param t_model: transformer model\n",
    "    :param tokenizer: tokenizer object\n",
    "    :param data: input data to be used for training\n",
    "    :param emoji_tagging: flag if we should apply emoji tagging\n",
    "    :param emoji_threshold: threshold for selection of emojis to be added\n",
    "    \"\"\"\n",
    "    processed = dict()\n",
    "    \n",
    "    # select words to be added\n",
    "    words_to_add = [\"[NAME]\", \"[RELIGION]\"]\n",
    "    if emoji_tagging:\n",
    "        words_to_add += EMOJIS_FOUND.loc[lambda f: f[\"cum_pct\"] <= emoji_threshold].emoji.to_list()\n",
    "        \n",
    "    # add the new tokens\n",
    "    tokenizer.add_tokens(words_to_add)\n",
    "    t_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # for each group of data\n",
    "    for group in tqdm([\"train\", \"validation\", \"test\"]):\n",
    "        # pivot the adtaset and extract the emotions\n",
    "        df = data.loc[lambda f: f[\"set\"] == group].pivot_table(\n",
    "            index=[\"code\", \"text\"],\n",
    "            columns=[\"goemotion\"],\n",
    "            values=\"set\",\n",
    "            aggfunc=\"count\",\n",
    "            fill_value=0\n",
    "        ).reset_index().drop(columns=[\"none\"], errors=\"ignore\")\n",
    "\n",
    "        processed[group] = dict()\n",
    "        \n",
    "        # apply the tokenizer to the data\n",
    "        processed[group][\"tokens\"] = tokenizer(\n",
    "            df.text.to_list(), \n",
    "            max_length=MAX_SEQUENCE_LENGTH, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        \n",
    "        # create same inputs to be used in the model\n",
    "        processed[group][\"inputs\"] = [\n",
    "            processed[group][\"tokens\"].input_ids, \n",
    "            processed[group][\"tokens\"].token_type_ids, \n",
    "            processed[group][\"tokens\"].attention_mask\n",
    "        ]\n",
    "        \n",
    "        # create the set of labels\n",
    "        processed[group][\"labels\"] = df.iloc[:, 2:].values\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4763186",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "939daff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:27.962586Z",
     "start_time": "2023-07-30T12:14:27.948588Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_cls_model(\n",
    "    t_model: TFAutoModel,\n",
    "    trainable: str = \"all\",\n",
    "    head: str = \"none\",\n",
    "    dropout: float = 0.3,\n",
    "    label_smoothing: float = 0.1,\n",
    "    hidden_size: int = 256, \n",
    "    hidden_layers: list = [256, 128, 64],\n",
    "    dropout_layers: list = [0.3, 0.3, 0.3],\n",
    "    num_filters: list = [100, 100, 50, 25],\n",
    "    kernel_sizes: list = [2, 3, 4, 5],\n",
    "    learning_rate: float = 0.00005,\n",
    "    epsilon: float = 1e-08,\n",
    "    num_classes: int = 28,\n",
    "    max_sequence_length: int = MAX_SEQUENCE_LENGTH,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a classification model using a pre-trained transformer\n",
    "    \n",
    "    :param t_model: transformer model\n",
    "    :param trainable: select whith parts of the transformer model are trainable (all, last, none)\n",
    "    :param head: type of head to be applied (none, dense, mlp, cnn)\n",
    "    :param dropout: dropout value to be selected\n",
    "    :param label_smoothing: label smoothing to be applied\n",
    "    :param hidden_size: number of nodes for head=dense\n",
    "    :param hidden_layers: number of nodes per layer for head=mlp\n",
    "    :param dropout_layers: dropout rate for each hidden layer for head=mlp\n",
    "    :param num_filters: number of filters to use for head=cnn\n",
    "    :param kernel_sizes: kernal sizes to use for head=cnn\n",
    "    :param learning_rate: learning rate applied for Adam\n",
    "    :param epsilon: epsilon selected for Adam\n",
    "    :param num_classes: number of classes to predict\n",
    "    :param max_sequence_length: maximum sequence length selected\n",
    "    \"\"\"\n",
    "    # set the model to be trainable\n",
    "    if trainable == \"all\":\n",
    "        t_model.trainable = True\n",
    "    elif trainable == \"last\":\n",
    "        last_layer_num = max(\n",
    "            [\n",
    "                int(w.name[w.name.index(\"layer_._\"):].split(\"/\")[0].replace(\"layer_._\", \"\"))\n",
    "                for w in t_model.weights if \"layer_._\" in w.name\n",
    "            ]\n",
    "        )\n",
    "        for w in t_model.weights:\n",
    "            if f\"layer_._{last_layer_num}\" not in w.name:\n",
    "                w._trainable = False\n",
    "    elif trainable == \"none\":\n",
    "        t_model.trainable = False\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # extract input ids, token ids and the attention mask\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name=\"input_ids_layer\")\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name=\"token_type_ids_layer\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name=\"attention_mask_layer\")\n",
    "    \n",
    "    # encode this into the model\n",
    "    model_inputs = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}      \n",
    "    output = t_model(model_inputs)\n",
    "    \n",
    "    # if no head was selected, pass the pooler token to a dropout layer\n",
    "    if head == \"none\":\n",
    "        pooler_token = output[1]\n",
    "        hidden = tf.keras.layers.Dropout(dropout)(pooler_token)\n",
    "        \n",
    "    # if a dense head was selected, add a hidden layer with the selected hidden size\n",
    "    elif head == \"dense\":\n",
    "        pooler_token = output[1]\n",
    "        hidden = tf.keras.layers.Dense(hidden_size, activation=\"relu\", name=\"hidden_layer\")(pooler_token)\n",
    "        hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
    "    \n",
    "    # if multi-layer perceptron was selected, add the hidden layers on top of the pooler token\n",
    "    elif head == \"mlp\":\n",
    "        pooler_token = output[1]\n",
    "        hidden = tf.keras.layers.Dense(hidden_layers[0], activation=\"relu\", name=\"hidden_layer\")(pooler_token)\n",
    "        i = 0\n",
    "        for size in hidden_layers[1:]:\n",
    "            hidden = tf.keras.layers.Dropout(dropout_layers[i])(hidden)\n",
    "            hidden = tf.keras.layers.Dense(size, activation=\"relu\", name=\"hidden_layer\")(hidden)\n",
    "            i += 1\n",
    "        hidden = tf.keras.layers.Dropout(dropout_layers[-1])(hidden)\n",
    "    \n",
    "    # if cnn was selected, get the token embeddings and create a cnn network\n",
    "    elif head == \"cnn\":\n",
    "        token_embeddings = output[0]\n",
    "        cnn_outputs = []\n",
    "        for filters, kernel_size in zip(num_filters, kernel_sizes):\n",
    "            conv_layer = tf.keras.layers.Conv1D(\n",
    "                filters=filters, kernel_size=kernel_size, activation='relu'\n",
    "            )(token_embeddings)\n",
    "            max_pool = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "            cnn_outputs.append(max_pool)\n",
    "        cnn_concat = tf.keras.layers.concatenate(cnn_outputs)\n",
    "        \n",
    "        hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cnn_concat)\n",
    "        hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
    "    \n",
    "    # with the final hidden layer, add a dense layer for the classification task\n",
    "    classification = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"classification_layer\")(hidden)\n",
    "    \n",
    "    # instantiate the classification model\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
    "    \n",
    "    # compile using the learning rate and selected epsilon\n",
    "    classification_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing), \n",
    "        metrics=[tfa.metrics.F1Score(num_classes=num_classes, average=\"macro\", threshold=0.2)]\n",
    "    )\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc372e38",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca6e9edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:27.978585Z",
     "start_time": "2023-07-30T12:14:27.963586Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "TARGET_NAMES = sorted(list(dataset[\"goemotion\"].unique()))\n",
    "\n",
    "\n",
    "def evaluate_model(p_set: str, proba: pd.DataFrame, dataset: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the model results\n",
    "    \n",
    "    :param p_set: which prediction set to use (test, validation)\n",
    "    :param dataset: the complete dataset\n",
    "    :return: model evaluation values\n",
    "    \"\"\"\n",
    "    outputs = dict()\n",
    "    df = dataset.loc[lambda f: f[\"set\"] == p_set]\n",
    "    \n",
    "    # get the one-hot-encoded values\n",
    "    pv = df.pivot_table(index=[\"code\"], columns=\"goemotion\", values=\"set\", aggfunc=\"count\", fill_value=0)\n",
    "    \n",
    "    # generate the predictions\n",
    "    proba = proba.loc[pv.index, pv.columns]\n",
    "    predictions = (proba.values > 0.2).astype(int)\n",
    "    pred = (\n",
    "        proba.reset_index()\n",
    "        .rename(columns={\"index\": \"code\"})\n",
    "        .melt(id_vars=[\"code\"], var_name=\"goemotion\", value_name=\"proba\")\n",
    "    )\n",
    "    pred[\"flag\"] = pred[\"proba\"] > 0.2\n",
    "    outputs[\"predictions\"] = pred\n",
    "    \n",
    "    # calculate metrics\n",
    "    outputs[\"f1_macro\"] = f1_score(pv[TARGET_NAMES].values, predictions, average=\"macro\")\n",
    "    outputs[\"f1_micro\"] = f1_score(pv[TARGET_NAMES].values, predictions, average=\"micro\")\n",
    "    outputs[\"roc_auc\"] = roc_auc_score(pv[TARGET_NAMES].values, proba.values, average=\"macro\", multi_class=\"ovo\")\n",
    "    outputs[\"confusion_matrix\"] = confusion_matrix(\n",
    "        np.argmax(pv[TARGET_NAMES].values, axis=1), np.argmax(predictions, axis=1)\n",
    "    )\n",
    "    outputs[\"classification_report\"] = classification_report(\n",
    "        pv[TARGET_NAMES].values, predictions, target_names=TARGET_NAMES\n",
    "    )\n",
    "    \n",
    "    # get the misclassification value\n",
    "    df = df.merge(pred.loc[lambda f: f[\"flag\"] == 1], on=[\"code\", \"goemotion\"], how=\"left\")\n",
    "    corrclass = df[df[\"flag\"].notnull()]\n",
    "    misclass = df[df[\"flag\"].isnull()]\n",
    "    outputs[\"misclassification\"] = misclass\n",
    "    \n",
    "    # get misclassification examples\n",
    "    outputs[\"misclassification_examples\"] = {\n",
    "        label: misclass[misclass[\"goemotion\"] == label]\n",
    "        .sample(3, replace=True)\n",
    "        .drop_duplicates()\n",
    "        .text\n",
    "        .to_list()\n",
    "        for label in TARGET_NAMES\n",
    "        if misclass[misclass[\"goemotion\"] == label].shape[0] > 0\n",
    "    }\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c293d",
   "metadata": {},
   "source": [
    "### Scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6525fa29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:27.994588Z",
     "start_time": "2023-07-30T12:14:27.979585Z"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler_10(epoch, lr):\n",
    "    return lr\n",
    "\n",
    "\n",
    "def scheduler_05(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "    \n",
    "def scheduler_02(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * 0.2\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "    \n",
    "def scheduler_exp(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638956db",
   "metadata": {},
   "source": [
    "### Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c214d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:28.072633Z",
     "start_time": "2023-07-30T12:14:27.996585Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import ipynbname\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "NB_FNAME = ipynbname.name()\n",
    "\n",
    "SCHEDULERS = {\"1.0\": scheduler_10, \"0.5\": scheduler_05, \"0.2\": scheduler_02, \"exp\": scheduler_exp}\n",
    "\n",
    "\n",
    "def limit_mem():\n",
    "    tf.compat.v1.keras.backend.get_session().close()\n",
    "    cfg = tf.compat.v1.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=cfg))\n",
    "    \n",
    "\n",
    "\n",
    "def run_model_experiment(grid: dict, dataset: pd.DataFrame):   \n",
    "    # select the appropriate dataset based on the balancing parameter\n",
    "    if grid[\"balancing\"] != \"none\":\n",
    "        data_for_exp = apply_balancing(dataset, grid[\"balancing\"], grid[\"augment\"])\n",
    "    else:\n",
    "        data_for_exp = dataset.copy()\n",
    "\n",
    "    # select the combinations to execute\n",
    "    combinations = {k: (k,) for k in data_for_exp.goemotion.unique()}\n",
    "    if len(grid[\"minority_shuffling\"]) > 0:\n",
    "        combinations = grid[\"minority_shuffling\"]\n",
    "\n",
    "    # apply cleaning to the dataset\n",
    "    if grid[\"clean_data\"]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # get the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(grid[\"model\"])\n",
    "    try:\n",
    "        t_model = TFAutoModel.from_pretrained(grid[\"model\"])\n",
    "    except OSError:\n",
    "        t_model = TFAutoModel.from_pretrained(grid[\"model\"], from_pt=True)\n",
    "\n",
    "    # get how many model replacements we should make\n",
    "    replaces = [{v: k for k, vl in combinations.items() for v in vl}]\n",
    "    for k, vl in combinations.items():\n",
    "        if len(vl) > 1:\n",
    "            replaces.append({v: v for v in vl})\n",
    "            \n",
    "    # create the folder\n",
    "    folder = Path(\"../experiments/\" + pd.to_datetime(\"today\").strftime(\"%Y%m%dT%H%M%S\"))\n",
    "    folder.mkdir(exist_ok=False, parents=True)\n",
    "\n",
    "    # run the model loop\n",
    "    predictions = dict()\n",
    "    for i, comb in enumerate(replaces):\n",
    "        # select the data to be used for training\n",
    "        data_for_training = (\n",
    "            data_for_exp.loc[lambda f: ((f[\"goemotion\"].isin(comb)) & (f[\"set\"] == \"train\")) | (f[\"set\"] != \"train\")]\n",
    "            .assign(goemotion=lambda f: f[\"goemotion\"].apply(lambda x: \"none\" if x not in comb else x))\n",
    "            .assign(goemotion=lambda f: f[\"goemotion\"].replace(comb))\n",
    "        )\n",
    "\n",
    "        # apply tokenization\n",
    "        processed = apply_tokenization(t_model, tokenizer, data_for_training, grid[\"emoji_tagging\"])\n",
    "\n",
    "        # create the model\n",
    "        cls_model = create_cls_model(\n",
    "            t_model, \n",
    "            trainable=grid[\"trainable\"], \n",
    "            head=grid[\"head\"], \n",
    "            dropout=grid.get(\"dropout\", 0.3),\n",
    "            label_smoothing=grid[\"label_smoothing\"],\n",
    "            learning_rate=grid[\"learning_rate\"],\n",
    "            num_classes=data_for_training.loc[lambda f: f[\"set\"] == \"train\"].goemotion.nunique(),\n",
    "        )\n",
    "        \n",
    "        # get the indexes that make the validation set\n",
    "        indexes = (\n",
    "            data_for_training.loc[lambda f: f[\"set\"] == \"validation\"]\n",
    "            .pivot_table(index=\"code\", columns=[\"goemotion\"], values=\"set\", aggfunc=\"count\", fill_value=0)\n",
    "            .drop(columns=[\"none\"], errors=\"ignore\")\n",
    "            .sum(axis=1)\n",
    "            .reset_index()\n",
    "            .loc[lambda f: f[0] > 0]\n",
    "            .index\n",
    "        )\n",
    "        val_inputs = [tf.convert_to_tensor(tensor.numpy()[indexes]) for tensor in processed[\"validation\"][\"inputs\"]]\n",
    "        val_labels = processed[\"validation\"][\"labels\"][indexes]\n",
    "\n",
    "        # fit the model\n",
    "        cb_scheduler = tf.keras.callbacks.LearningRateScheduler(SCHEDULERS[grid[\"scheduler\"]])\n",
    "        cb_earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model_history = cls_model.fit(\n",
    "            processed[\"train\"][\"inputs\"],\n",
    "            processed[\"train\"][\"labels\"],\n",
    "            validation_data=(val_inputs, val_labels),\n",
    "            batch_size=grid[\"batch_size\"],\n",
    "            epochs=grid[\"epochs\"],\n",
    "            callbacks=[cb_scheduler, cb_earlystop],\n",
    "        )\n",
    "        \n",
    "        # save the history data\n",
    "        with open(folder / f\"history_{i}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(model_history.history, f)\n",
    "\n",
    "        # run the probability calculation\n",
    "        classes = sorted(list(set(comb.values())))\n",
    "        predictions[tuple(classes)] = dict()\n",
    "        for p_set in [\"validation\", \"test\"]:\n",
    "            index = (\n",
    "                data_for_training.loc[lambda f: f[\"set\"] == p_set]\n",
    "                .pivot_table(index=\"code\", columns=[\"goemotion\"], values=\"set\", aggfunc=\"count\", fill_value=0)\n",
    "                .index\n",
    "            )\n",
    "            predictions[tuple(classes)][p_set] = pd.DataFrame(\n",
    "                cls_model.predict(processed[p_set][\"inputs\"]), index=index, columns=classes,\n",
    "            )\n",
    "    \n",
    "    # export the base predictions\n",
    "    with open(folder / \"original_predictions.pkl\", \"wb\") as f:\n",
    "        pickle.dump(predictions, f)\n",
    "    \n",
    "    # ensure for the post predictions that we have the expected values\n",
    "    base = predictions[list(predictions)[0]]\n",
    "    for k, vl in combinations.items():\n",
    "        if len(vl) > 1:\n",
    "            values = predictions[tuple(sorted(vl))]\n",
    "            for p_set in [\"validation\", \"test\"]:\n",
    "                base[p_set] = pd.concat(\n",
    "                    [base[p_set].drop(columns=k), values[p_set].multiply(base[p_set][k], axis=0)], axis=1\n",
    "                )\n",
    "\n",
    "    # run the model evaluation on validation\n",
    "    val_res = evaluate_model(\"validation\", base[\"validation\"], data_for_exp)\n",
    "    test_res = evaluate_model(\"test\", base[\"test\"], data_for_exp)\n",
    "    \n",
    "    # save the model val_res\n",
    "    with open(folder / \"grid_params.json\", \"w\") as f:\n",
    "        json.dump(grid, f)\n",
    "    \n",
    "    for res, name in [(val_res, \"validation\"), (test_res, \"test\")]:\n",
    "        with open(folder / f\"metrics_{name}.json\", \"w\") as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    r: v \n",
    "                    for r, v in res.items() \n",
    "                    if r not in [\"confusion_matrix\", \"predictions\", \"misclassification\"]\n",
    "                }, \n",
    "                f\n",
    "            )\n",
    "        with open(folder / f\"confusion_matrix_{name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(res[\"confusion_matrix\"], f)\n",
    "\n",
    "        res[\"predictions\"].to_pickle(folder / f\"prediction_{name}.pkl\")\n",
    "        res[\"misclassification\"].to_pickle(folder / f\"misclassification_{name}.pkl\")\n",
    "    \n",
    "    data_for_exp.to_pickle(folder / \"data_for_exp.pkl\")\n",
    "    \n",
    "    shutil.copyfile(os.path.abspath(f\"{NB_FNAME}.ipynb\"), folder / f\"{NB_FNAME}.ipynb\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "966227d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T00:34:44.000330Z",
     "start_time": "2023-07-30T00:32:58.291018Z"
    }
   },
   "source": [
    "np.random.seed(33)\n",
    "tf.random.set_seed(33)\n",
    "\n",
    "grid = {\n",
    "    \"model\": \"bert-base-uncased\",\n",
    "    \"trainable\": \"last\",\n",
    "    \"head\": \"none\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"scheduler\": \"1.0\",\n",
    "    \"dropout\": 0.3,\n",
    "    \"label_smoothing\": 0.05,\n",
    "    \"learning_rate\": 0.00005,\n",
    "    \"emoji_tagging\": True,\n",
    "    \"clean_data\": False,\n",
    "    \"balancing\": \"none\",\n",
    "    \"augment\": False,\n",
    "    \"minority_shuffling\": {\n",
    "        \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "        \"nervousness\": (\"nervousness\", ),\n",
    "        \"fear\": (\"fear\", ),\n",
    "        \"pride\": (\"pride\", ),\n",
    "        \"approval\": (\"approval\", ),\n",
    "        \"realization\": (\"realization\", ),\n",
    "        \"surprise\": (\"surprise\", ),\n",
    "        \"relief\": (\"relief\",),\n",
    "        \"joy\": (\"joy\",),\n",
    "        \"neutral\": (\"neutral\",),\n",
    "        \"optimism\": (\"optimism\",),\n",
    "        \"anger\": (\"anger\",),\n",
    "        \"desire\": (\"desire\",),\n",
    "        \"love\": (\"love\",),\n",
    "        \"disapproval\": (\"disapproval\",),\n",
    "        \"amusement\": (\"amusement\",),\n",
    "        \"caring\": (\"caring\",),\n",
    "        \"excitement\": (\"excitement\",),\n",
    "        \"curiosity\": (\"curiosity\",),\n",
    "        \"embarrassment\": (\"embarrassment\",),\n",
    "        \"disgust\": (\"disgust\",),\n",
    "        \"gratitude\": (\"gratitude\",),\n",
    "        \"annoyance\": (\"annoyance\",),\n",
    "        \"confusion\": (\"confusion\",),\n",
    "        \"disappointment\": (\"disappointment\",),\n",
    "        \"admiration\": (\"admiration\",),\n",
    "        \"remorse\": (\"remorse\",)\n",
    "    },\n",
    "}\n",
    "\n",
    "limit_mem()\n",
    "tf.keras.backend.clear_session()\n",
    "while gc.collect():\n",
    "    continue\n",
    "run_model_experiment(grid, dataset.sample(frac=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f321ad8b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225f6ad",
   "metadata": {},
   "source": [
    "## Grid Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "417f89fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:28.104751Z",
     "start_time": "2023-07-30T12:14:28.073586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160 searches created - expected time: 500.0 days\n"
     ]
    }
   ],
   "source": [
    "GRID = {\n",
    "    \"model\": [\n",
    "        \"bert-base-uncased\",\n",
    "        \"vinai/bertweet-base\",\n",
    "        \"flboehm/reddit-bert-text_10\",\n",
    "    ],\n",
    "    \"trainable\": [\"all\", \"last\", \"none\"],\n",
    "    \"head\": [\"none\", \"dense\", \"cnn\", \"mlp\"],\n",
    "    \"dropout\": [0.3],\n",
    "    \"label_smoothing\": [0, 0.1],\n",
    "    \"epochs\": [10],\n",
    "    \"batch_size\": [8, 16],\n",
    "    \"learning_rate\": [0.00005],\n",
    "    \"emoji_tagging\": [False, True],\n",
    "    \"clean_data\": [False, True],\n",
    "    \"balancing\": [\"none\", \"avg\"],\n",
    "    \"augment\": [False, True],\n",
    "    \"minority_shuffling\": [\n",
    "        dict(),\n",
    "        {\n",
    "            \"neutral\": [\"neutral\"],\n",
    "            \"anger\": [\"anger\", \"annoyance\", \"disapproval\"],\n",
    "            \"disgust\": [\"disgust\"],\n",
    "            \"fear\": [\"fear\", \"nervousness\"],\n",
    "            \"joy\": [\n",
    "                \"joy\", \n",
    "                \"amusement\",\n",
    "                \"approval\", \n",
    "                \"excitement\", \n",
    "                \"gratitude\",  \n",
    "                \"love\", \n",
    "                \"optimism\", \n",
    "                \"relief\", \n",
    "                \"pride\", \n",
    "                \"admiration\", \n",
    "                \"desire\", \n",
    "                \"caring\"\n",
    "            ],\n",
    "            \"sadness\": [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\",  \"remorse\"],\n",
    "            \"surprise\": [\"surprise\", \"realization\", \"confusion\", \"curiosity\"]\n",
    "        },\n",
    "        {\n",
    "            \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "            \"nervousness_fear\": (\"nervousness\", \"fear\"),\n",
    "            \"pride_approval\": (\"pride\", \"approval\"),\n",
    "            \"realization_surprise\": (\"realization\", \"surprise\"),\n",
    "            \"relief_joy\": (\"relief\", \"joy\"),\n",
    "            \"neutral\": (\"neutral\",),\n",
    "            \"optimism\": (\"optimism\",),\n",
    "            \"anger\": (\"anger\",),\n",
    "            \"desire\": (\"desire\",),\n",
    "            \"love\": (\"love\",),\n",
    "            \"disapproval\": (\"disapproval\",),\n",
    "            \"amusement\": (\"amusement\",),\n",
    "            \"caring\": (\"caring\",),\n",
    "            \"excitement\": (\"excitement\",),\n",
    "            \"curiosity\": (\"curiosity\",),\n",
    "            \"embarrassment\": (\"embarrassment\",),\n",
    "            \"disgust\": (\"disgust\",),\n",
    "            \"gratitude\": (\"gratitude\",),\n",
    "            \"annoyance\": (\"annoyance\",),\n",
    "            \"confusion\": (\"confusion\",),\n",
    "            \"disappointment\": (\"disappointment\",),\n",
    "            \"admiration\": (\"admiration\",),\n",
    "            \"remorse\": (\"remorse\",)\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "EARLY_STOP = 0.55\n",
    "\n",
    "keys, values = zip(*GRID.items())\n",
    "grid_space = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "valid_space = list()\n",
    "for g in grid_space:\n",
    "    if g[\"clean_data\"] and not g[\"emoji_tagging\"]:\n",
    "        continue\n",
    "    if g[\"augment\"] and g[\"balancing\"] == \"none\":\n",
    "        continue\n",
    "    if len(g[\"minority_shuffling\"]) > 0 and g[\"balancing\"] != \"none\":\n",
    "        continue\n",
    "    if len(g[\"minority_shuffling\"]) > 0 and g[\"augment\"]:\n",
    "        continue\n",
    "    valid_space.append(g)\n",
    "    \n",
    "print(len(valid_space), \"searches created - expected time:\", len(valid_space) * 10000 * 2 / (24 * 60 * 60), \"days\")\n",
    "for _ in range(10):\n",
    "    random.shuffle(valid_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c454ba6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T12:14:28.120696Z",
     "start_time": "2023-07-30T12:14:28.105691Z"
    }
   },
   "outputs": [],
   "source": [
    "EKMAN_SAMPLING = {\n",
    "    \"neutral\": (\"neutral\", ),\n",
    "    \"anger\": (\"anger\", \"annoyance\", \"disapproval\"),\n",
    "    \"disgust\": (\"disgust\", ),\n",
    "    \"fear\": (\"fear\", \"nervousness\"),\n",
    "    \"joy\": (\n",
    "        \"joy\", \n",
    "        \"amusement\",\n",
    "        \"approval\", \n",
    "        \"excitement\", \n",
    "        \"gratitude\",  \n",
    "        \"love\", \n",
    "        \"optimism\", \n",
    "        \"relief\", \n",
    "        \"pride\", \n",
    "        \"admiration\", \n",
    "        \"desire\", \n",
    "        \"caring\"\n",
    "    ),\n",
    "    \"sadness\": (\"sadness\", \"disappointment\", \"embarrassment\", \"grief\",  \"remorse\"),\n",
    "    \"surprise\": (\"surprise\", \"realization\", \"confusion\", \"curiosity\")\n",
    "}\n",
    "\n",
    "SELECTED_SAMPLING_1 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"nervousness\": (\"nervousness\",),\n",
    "    \"fear\": (\"fear\", ),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"relief\": (\"relief\",),\n",
    "    \"joy\": (\"joy\",),\n",
    "    \"neutral\": (\"neutral\",),\n",
    "    \"optimism\": (\"optimism\",),\n",
    "    \"anger\": (\"anger\",),\n",
    "    \"desire\": (\"desire\",),\n",
    "    \"love\": (\"love\",),\n",
    "    \"disapproval\": (\"disapproval\",),\n",
    "    \"amusement\": (\"amusement\",),\n",
    "    \"caring\": (\"caring\",),\n",
    "    \"excitement\": (\"excitement\",),\n",
    "    \"curiosity\": (\"curiosity\",),\n",
    "    \"embarrassment\": (\"embarrassment\",),\n",
    "    \"disgust\": (\"disgust\",),\n",
    "    \"gratitude\": (\"gratitude\",),\n",
    "    \"annoyance\": (\"annoyance\",),\n",
    "    \"confusion\": (\"confusion\",),\n",
    "    \"disappointment\": (\"disappointment\",),\n",
    "    \"remorse\": (\"remorse\",)\n",
    "}\n",
    "\n",
    "SELECTED_SAMPLING_2 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"nervousness_fear\": (\"nervousness\", \"fear\"),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"relief\": (\"relief\",),\n",
    "    \"joy\": (\"joy\",),\n",
    "    \"neutral\": (\"neutral\",),\n",
    "    \"optimism\": (\"optimism\",),\n",
    "    \"anger\": (\"anger\",),\n",
    "    \"desire\": (\"desire\",),\n",
    "    \"love\": (\"love\",),\n",
    "    \"disapproval\": (\"disapproval\",),\n",
    "    \"amusement\": (\"amusement\",),\n",
    "    \"caring\": (\"caring\",),\n",
    "    \"excitement\": (\"excitement\",),\n",
    "    \"curiosity\": (\"curiosity\",),\n",
    "    \"embarrassment\": (\"embarrassment\",),\n",
    "    \"disgust\": (\"disgust\",),\n",
    "    \"gratitude\": (\"gratitude\",),\n",
    "    \"annoyance\": (\"annoyance\",),\n",
    "    \"confusion\": (\"confusion\",),\n",
    "    \"disappointment\": (\"disappointment\",),\n",
    "    \"remorse\": (\"remorse\",)\n",
    "}\n",
    "\n",
    "SELECTED_SAMPLING_3 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"annoyance_anger\": (\"annoyance\", \"anger\"),\n",
    "    \"nervousness\": (\"nervousness\",),\n",
    "    \"fear\": (\"fear\", )\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"relief\": (\"relief\",),\n",
    "    \"joy\": (\"joy\",),\n",
    "    \"neutral\": (\"neutral\",),\n",
    "    \"optimism\": (\"optimism\",),\n",
    "    \"desire\": (\"desire\",),\n",
    "    \"love\": (\"love\",),\n",
    "    \"disapproval\": (\"disapproval\",),\n",
    "    \"amusement\": (\"amusement\",),\n",
    "    \"caring\": (\"caring\",),\n",
    "    \"excitement\": (\"excitement\",),\n",
    "    \"curiosity\": (\"curiosity\",),\n",
    "    \"embarrassment\": (\"embarrassment\",),\n",
    "    \"disgust\": (\"disgust\",),\n",
    "    \"gratitude\": (\"gratitude\",),\n",
    "    \"confusion\": (\"confusion\",),\n",
    "    \"disappointment\": (\"disappointment\",),\n",
    "    \"remorse\": (\"remorse\",)\n",
    "}\n",
    "\n",
    "SELECTED_SAMPLING_4 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"annoyance_anger\": (\"annoyance\", \"anger\"),\n",
    "    \"nervousness_fear\": (\"nervousness\", \"fear\"),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"relief\": (\"relief\",),\n",
    "    \"joy\": (\"joy\",),\n",
    "    \"neutral\": (\"neutral\",),\n",
    "    \"optimism\": (\"optimism\",),\n",
    "    \"desire\": (\"desire\",),\n",
    "    \"love\": (\"love\",),\n",
    "    \"disapproval\": (\"disapproval\",),\n",
    "    \"amusement\": (\"amusement\",),\n",
    "    \"caring\": (\"caring\",),\n",
    "    \"excitement\": (\"excitement\",),\n",
    "    \"curiosity\": (\"curiosity\",),\n",
    "    \"embarrassment\": (\"embarrassment\",),\n",
    "    \"disgust\": (\"disgust\",),\n",
    "    \"gratitude\": (\"gratitude\",),\n",
    "    \"confusion\": (\"confusion\",),\n",
    "    \"disappointment\": (\"disappointment\",),\n",
    "    \"remorse\": (\"remorse\",)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494346e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a3019",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429dee4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-30T12:14:18.281Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pedro.forli\\AppData\\Local\\Temp\\ipykernel_27172\\2637764286.py:16: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2714/2714 [==============================] - 1373s 501ms/step - loss: 2.1724 - f1_score: 0.4151 - val_loss: 1.7858 - val_f1_score: 0.5183 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "2714/2714 [==============================] - 1284s 473ms/step - loss: 1.6983 - f1_score: 0.5459 - val_loss: 1.8089 - val_f1_score: 0.5321 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "2267/2714 [========================>.....] - ETA: 3:20 - loss: 1.5009 - f1_score: 0.5964"
     ]
    }
   ],
   "source": [
    "np.random.seed(33)\n",
    "tf.random.set_seed(33)\n",
    "\n",
    "for grid in [\n",
    "    {\n",
    "        \"model\": \"bert-base-uncased\",\n",
    "        \"trainable\": \"all\",\n",
    "        \"head\": \"none\",\n",
    "        \"scheduler\": \"0.2\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"label_smoothing\": 0,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.00005,\n",
    "        \"emoji_tagging\": True,\n",
    "        \"clean_data\": False,\n",
    "        \"balancing\": \"none\",\n",
    "        \"augment\": False,\n",
    "        \"minority_shuffling\": SELECTED_SAMPLING_2,\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"roberta-base\",\n",
    "        \"trainable\": \"all\",\n",
    "        \"head\": \"none\",\n",
    "        \"scheduler\": \"0.2\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"label_smoothing\": 0,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.00005,\n",
    "        \"emoji_tagging\": True,\n",
    "        \"clean_data\": False,\n",
    "        \"balancing\": \"none\",\n",
    "        \"augment\": False,\n",
    "        \"minority_shuffling\": SELECTED_SAMPLING_1,\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"roberta-base\",\n",
    "        \"trainable\": \"all\",\n",
    "        \"head\": \"none\",\n",
    "        \"scheduler\": \"0.2\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"label_smoothing\": 0,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.00005,\n",
    "        \"emoji_tagging\": True,\n",
    "        \"clean_data\": False,\n",
    "        \"balancing\": \"none\",\n",
    "        \"augment\": False,\n",
    "        \"minority_shuffling\": SELECTED_SAMPLING_2,\n",
    "    },\n",
    "]:\n",
    "    limit_mem()\n",
    "    tf.keras.backend.clear_session()\n",
    "    while gc.collect():\n",
    "        continue\n",
    "    run_model_experiment(grid, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd15b4",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
