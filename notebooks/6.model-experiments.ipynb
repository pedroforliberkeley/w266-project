{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205a9e54",
   "metadata": {},
   "source": [
    "# Balancing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5e36e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4782f5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e281e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:41.495116Z",
     "start_time": "2023-08-01T21:30:34.466913Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412bec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:41.510118Z",
     "start_time": "2023-08-01T21:30:41.497106Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b6bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:41.526105Z",
     "start_time": "2023-08-01T21:30:41.511106Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db02cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:41.572079Z",
     "start_time": "2023-08-01T21:30:41.528106Z"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e6e9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:41.588066Z",
     "start_time": "2023-08-01T21:30:41.573066Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0aca1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e55a9e",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c392e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:41.743428Z",
     "start_time": "2023-08-01T21:30:41.589067Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(\"../data/clean_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5a9f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71775a39",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762431da",
   "metadata": {},
   "source": [
    "### Balancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e2ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.019116Z",
     "start_time": "2023-08-01T21:30:41.744428Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "\n",
    "\n",
    "SR = random.SystemRandom()\n",
    "LANGUAGES = [\"es\", \"de\", \"fr\", \"ar\", \"te\", \"hi\", \"ja\", \"fa\", \"sq\", \"bg\", \"nl\", \"gu\", \"ig\", \"kk\", \"mt\", \"ps\"]\n",
    "TRANSLATIONS = dict()\n",
    "\n",
    "\n",
    "def data_augmentation(message: str, language: str = \"en\", aug_range: int = 1) -> list:\n",
    "    \"\"\"\n",
    "    Create new text data by translating a message to a random language\n",
    "    and then tranlating it back\n",
    "    \n",
    "    :param message: messege to be translated\n",
    "    :param language: original language of the message\n",
    "    :param aug_range: number of new messages to generate\n",
    "    :return: list of new messages\n",
    "    \"\"\"\n",
    "    augmented_messages = []\n",
    "    if hasattr(message, \"decode\"):\n",
    "        message = message.decode(\"utf-8\")\n",
    "\n",
    "    for j in range(0, aug_range) :\n",
    "        text_blob = TextBlob(message)\n",
    "        try:\n",
    "            to_lang = SR.choice(LANGUAGES)\n",
    "            \n",
    "            if (message, to_lang) in TRANSLATIONS:\n",
    "                text_blob = TRANSLATIONS[(message, to_lang)]\n",
    "            else:\n",
    "                text_blob = text_blob.translate(from_lang=language, to=to_lang)\n",
    "                text_blob = text_blob.translate(from_lang=to_lang, to=language)\n",
    "                TRANSLATIONS[(message, to_lang)] = str(text_blob)\n",
    "        except NotTranslated:\n",
    "            pass\n",
    "        else:\n",
    "            augmented_messages.append(TRANSLATIONS[(message, to_lang)])\n",
    "\n",
    "    return augmented_messages\n",
    "\n",
    "\n",
    "def apply_balancing(data: pd.DataFrame, target: str, augmented: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply balancing to the dataset based on the selected strategy\n",
    "    \n",
    "    :param data: dataset to be balanced\n",
    "    :param target: target strategy (avg or max)\n",
    "    :param augmented: flag if we should apply augmentation (back-and-forth translation)\n",
    "    :return: balanced dataframe\n",
    "    \"\"\"\n",
    "    assert (augmented and target == \"avg\") or not augmented\n",
    "    \n",
    "    # get the file name with the augmented data\n",
    "    if augmented:\n",
    "        file_name = f\"balanced_augmented_{target}.parquet\"\n",
    "    else:\n",
    "        file_name = f\"balanced_{target}.parquet\"\n",
    "    \n",
    "    # if the file exists load and returns it\n",
    "    if os.path.exists(f\"../data/{file_name}\"):\n",
    "        return pd.read_parquet(f\"../data/{file_name}\")\n",
    "    \n",
    "    # select the training data\n",
    "    train = data.loc[lambda f: f[\"set\"] == \"train\"].copy()\n",
    "    train[\"augmented\"] = False\n",
    "    train[\"for_balance\"] = False\n",
    "    \n",
    "    # calculate the representation of each class\n",
    "    class_representation = train.goemotion.value_counts()\n",
    "    minority = class_representation.min()\n",
    "    majority = class_representation.max()\n",
    "    avg = int(class_representation.mean())\n",
    "\n",
    "    # choose what is the target amount of each class based on the strategy\n",
    "    if target == \"max\":\n",
    "        target_value = majority\n",
    "    elif target == \"avg\":\n",
    "        target_value = avg\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    # for each emotion of interest\n",
    "    final = list()\n",
    "    for emotion in class_representation.index:\n",
    "        # get the data with that emotion\n",
    "        edata = train.loc[lambda f: f[\"goemotion\"] == emotion]\n",
    "        representation = class_representation.loc[emotion]\n",
    "        to_generate = target_value - representation\n",
    "        \n",
    "        # if we have less than the target value, we should balance it\n",
    "        if representation < target_value:\n",
    "            # if we don't want any augmentation, do a simple sampling of the data\n",
    "            if not augmented:\n",
    "                sampled = pd.concat([edata, edata.sample(to_generate, replace=True).assign(for_balance=True)])\n",
    "            \n",
    "            # otherwise\n",
    "            else:\n",
    "                # sample some text from the original dataset\n",
    "                generated = list()\n",
    "                sampled = edata.sample(to_generate, replace=(to_generate > representation)).reset_index(drop=True)\n",
    "                \n",
    "                # for each text in the sample\n",
    "                for row in tqdm(sampled.itertuples(name=None), total=sampled.shape[0]):\n",
    "                    # try to perform the translation of text at least 3 times\n",
    "                    for i in range(3):\n",
    "                        try:\n",
    "                            sampled.loc[row[0], \"text\"] = data_augmentation(row[-4])[0]\n",
    "                            sampled.loc[row[0], \"augmented\"] = True\n",
    "                        except IndexError:\n",
    "                            continue\n",
    "                        except urllib.error.URLError:\n",
    "                            time.sleep(3)\n",
    "                        else:\n",
    "                            break\n",
    "                \n",
    "                # save the final dataset\n",
    "                sampled[\"for_balance\"] = True\n",
    "                sampled = pd.concat([edata, sampled])\n",
    "        else:\n",
    "            sampled = edata\n",
    "        \n",
    "        # save the sampled results to the final dataset\n",
    "        final.append(sampled)\n",
    "\n",
    "    # concatenate with the original test set\n",
    "    balanced = pd.concat(final + [data.loc[lambda f: f[\"set\"] != \"train\"].assign(augmented=False, for_balance=False)])\n",
    "    balanced = (\n",
    "        balanced.sort_values(by=[\"set\", \"code\", \"goemotion\"])\n",
    "        .assign(count=lambda f: f.groupby([\"code\", \"goemotion\"])[\"for_balance\"].cumsum().values)\n",
    "        .assign(code=lambda f: np.where(f[\"count\"] > 0, f[\"code\"] + \"_\" + f[\"count\"].astype(str), f[\"code\"]))\n",
    "        .assign(text=lambda f: f[\"text\"].apply(lambda x: x.replace(\"[name]\", \"[NAME]\")))\n",
    "        .assign(text=lambda f: f[\"text\"].apply(lambda x: x.replace(\"name]\", \"[NAME]\")))\n",
    "        .assign(text=lambda f: f[\"text\"].apply(lambda x: x.replace(\"[name\", \"[NAME]\")))\n",
    "        .assign(text=lambda f: f[\"text\"].apply(lambda x: x.replace(\"[religion]\", \"[RELIGION]\")))\n",
    "        .drop(columns=\"count\")\n",
    "    )\n",
    "    \n",
    "    # export the final result\n",
    "    balanced.to_parquet(f\"../data/{file_name}\")\n",
    "    \n",
    "    \n",
    "    # return the sampled dataset\n",
    "    return balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a80497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.114115Z",
     "start_time": "2023-08-01T21:30:43.021117Z"
    }
   },
   "outputs": [],
   "source": [
    "apply_balancing(dataset, \"avg\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7326d",
   "metadata": {},
   "source": [
    "### Clean Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209145a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.177601Z",
     "start_time": "2023-08-01T21:30:43.115118Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import contractions\n",
    "import emoji\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "TOKENIZER = TweetTokenizer()\n",
    "STEEMER = PorterStemmer()\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "PUNCUATION_LIST = list(string.punctuation)\n",
    "\n",
    "\n",
    "ABBREVIATIONS = {\n",
    "    \"$\" : \" dollar \",\n",
    "    \"â‚¬\" : \" euro \",\n",
    "    \"4ao\" : \"for adults only\",\n",
    "    \"a.m\" : \"before midday\",\n",
    "    \"a3\" : \"anytime anywhere anyplace\",\n",
    "    \"aamof\" : \"as a matter of fact\",\n",
    "    \"acct\" : \"account\",\n",
    "    \"adih\" : \"another day in hell\",\n",
    "    \"afaic\" : \"as far as i am concerned\",\n",
    "    \"afaict\" : \"as far as i can tell\",\n",
    "    \"afaik\" : \"as far as i know\",\n",
    "    \"afair\" : \"as far as i remember\",\n",
    "    \"afk\" : \"away from keyboard\",\n",
    "    \"app\" : \"application\",\n",
    "    \"approx\" : \"approximately\",\n",
    "    \"apps\" : \"applications\",\n",
    "    \"asap\" : \"as soon as possible\",\n",
    "    \"asl\" : \"age, sex, location\",\n",
    "    \"atk\" : \"at the keyboard\",\n",
    "    \"ave.\" : \"avenue\",\n",
    "    \"aymm\" : \"are you my mother\",\n",
    "    \"ayor\" : \"at your own risk\", \n",
    "    \"b&b\" : \"bed and breakfast\",\n",
    "    \"b+b\" : \"bed and breakfast\",\n",
    "    \"b.c\" : \"before christ\",\n",
    "    \"b2b\" : \"business to business\",\n",
    "    \"b2c\" : \"business to customer\",\n",
    "    \"b4\" : \"before\",\n",
    "    \"b4n\" : \"bye for now\",\n",
    "    \"b@u\" : \"back at you\",\n",
    "    \"bae\" : \"before anyone else\",\n",
    "    \"bak\" : \"back at keyboard\",\n",
    "    \"bbbg\" : \"bye bye be good\",\n",
    "    \"bbc\" : \"british broadcasting corporation\",\n",
    "    \"bbias\" : \"be back in a second\",\n",
    "    \"bbl\" : \"be back later\",\n",
    "    \"bbs\" : \"be back soon\",\n",
    "    \"be4\" : \"before\",\n",
    "    \"bfn\" : \"bye for now\",\n",
    "    \"blvd\" : \"boulevard\",\n",
    "    \"bout\" : \"about\",\n",
    "    \"brb\" : \"be right back\",\n",
    "    \"bros\" : \"brothers\",\n",
    "    \"brt\" : \"be right there\",\n",
    "    \"bsaaw\" : \"big smile and a wink\",\n",
    "    \"btw\" : \"by the way\",\n",
    "    \"bwl\" : \"bursting with laughter\",\n",
    "    \"c/o\" : \"care of\",\n",
    "    \"cet\" : \"central european time\",\n",
    "    \"cf\" : \"compare\",\n",
    "    \"cia\" : \"central intelligence agency\",\n",
    "    \"csl\" : \"can not stop laughing\",\n",
    "    \"cu\" : \"see you\",\n",
    "    \"cul8r\" : \"see you later\",\n",
    "    \"cv\" : \"curriculum vitae\",\n",
    "    \"cwot\" : \"complete waste of time\",\n",
    "    \"cya\" : \"see you\",\n",
    "    \"cyt\" : \"see you tomorrow\",\n",
    "    \"dae\" : \"does anyone else\",\n",
    "    \"dbmib\" : \"do not bother me i am busy\",\n",
    "    \"diy\" : \"do it yourself\",\n",
    "    \"dm\" : \"direct message\",\n",
    "    \"dwh\" : \"during work hours\",\n",
    "    \"e123\" : \"easy as one two three\",\n",
    "    \"eet\" : \"eastern european time\",\n",
    "    \"eg\" : \"example\",\n",
    "    \"embm\" : \"early morning business meeting\",\n",
    "    \"encl\" : \"enclosed\",\n",
    "    \"encl.\" : \"enclosed\",\n",
    "    \"etc\" : \"and so on\",\n",
    "    \"faq\" : \"frequently asked questions\",\n",
    "    \"fawc\" : \"for anyone who cares\",\n",
    "    \"fb\" : \"facebook\",\n",
    "    \"fc\" : \"fingers crossed\",\n",
    "    \"fig\" : \"figure\",\n",
    "    \"fimh\" : \"forever in my heart\", \n",
    "    \"ft.\" : \"feet\",\n",
    "    \"ft\" : \"featuring\",\n",
    "    \"ftl\" : \"for the loss\",\n",
    "    \"ftw\" : \"for the win\",\n",
    "    \"fwiw\" : \"for what it is worth\",\n",
    "    \"fyi\" : \"for your information\",\n",
    "    \"g9\" : \"genius\",\n",
    "    \"gahoy\" : \"get a hold of yourself\",\n",
    "    \"gal\" : \"get a life\",\n",
    "    \"gcse\" : \"general certificate of secondary education\",\n",
    "    \"gfn\" : \"gone for now\",\n",
    "    \"gg\" : \"good game\",\n",
    "    \"gl\" : \"good luck\",\n",
    "    \"glhf\" : \"good luck have fun\",\n",
    "    \"gmt\" : \"greenwich mean time\",\n",
    "    \"gmta\" : \"great minds think alike\",\n",
    "    \"gn\" : \"good night\",\n",
    "    \"g.o.a.t\" : \"greatest of all time\",\n",
    "    \"goat\" : \"greatest of all time\",\n",
    "    \"goi\" : \"get over it\",\n",
    "    \"gps\" : \"global positioning system\",\n",
    "    \"gr8\" : \"great\",\n",
    "    \"gratz\" : \"congratulations\",\n",
    "    \"gyal\" : \"girl\",\n",
    "    \"h&c\" : \"hot and cold\",\n",
    "    \"hp\" : \"horsepower\",\n",
    "    \"hr\" : \"hour\",\n",
    "    \"hrh\" : \"his royal highness\",\n",
    "    \"ht\" : \"height\",\n",
    "    \"ibrb\" : \"i will be right back\",\n",
    "    \"ic\" : \"i see\",\n",
    "    \"icq\" : \"i seek you\",\n",
    "    \"icymi\" : \"in case you missed it\",\n",
    "    \"idc\" : \"i do not care\",\n",
    "    \"idgadf\" : \"i do not give a damn fuck\",\n",
    "    \"idgaf\" : \"i do not give a fuck\",\n",
    "    \"idk\" : \"i do not know\",\n",
    "    \"ie\" : \"that is\",\n",
    "    \"i.e\" : \"that is\",\n",
    "    \"ifyp\" : \"i feel your pain\",\n",
    "    \"IG\" : \"instagram\",\n",
    "    \"iirc\" : \"if i remember correctly\",\n",
    "    \"ilu\" : \"i love you\",\n",
    "    \"ily\" : \"i love you\",\n",
    "    \"imho\" : \"in my humble opinion\",\n",
    "    \"imo\" : \"in my opinion\",\n",
    "    \"imu\" : \"i miss you\",\n",
    "    \"iow\" : \"in other words\",\n",
    "    \"irl\" : \"in real life\",\n",
    "    \"j4f\" : \"just for fun\",\n",
    "    \"jic\" : \"just in case\",\n",
    "    \"jk\" : \"just kidding\",\n",
    "    \"jsyk\" : \"just so you know\",\n",
    "    \"l8r\" : \"later\",\n",
    "    \"lb\" : \"pound\",\n",
    "    \"lbs\" : \"pounds\",\n",
    "    \"ldr\" : \"long distance relationship\",\n",
    "    \"lmao\" : \"laugh my ass off\",\n",
    "    \"lmfao\" : \"laugh my fucking ass off\",\n",
    "    \"lol\" : \"laughing out loud\",\n",
    "    \"ltd\" : \"limited\",\n",
    "    \"ltns\" : \"long time no see\",\n",
    "    \"m8\" : \"mate\",\n",
    "    \"mf\" : \"motherfucker\",\n",
    "    \"mfs\" : \"motherfuckers\",\n",
    "    \"mfw\" : \"my face when\",\n",
    "    \"mofo\" : \"motherfucker\",\n",
    "    \"mph\" : \"miles per hour\",\n",
    "    \"mr\" : \"mister\",\n",
    "    \"mrw\" : \"my reaction when\",\n",
    "    \"ms\" : \"miss\",\n",
    "    \"mte\" : \"my thoughts exactly\",\n",
    "    \"nagi\" : \"not a good idea\",\n",
    "    \"nbc\" : \"national broadcasting company\",\n",
    "    \"nbd\" : \"not big deal\",\n",
    "    \"nfs\" : \"not for sale\",\n",
    "    \"ngl\" : \"not going to lie\",\n",
    "    \"nhs\" : \"national health service\",\n",
    "    \"nrn\" : \"no reply necessary\",\n",
    "    \"nsfl\" : \"not safe for life\",\n",
    "    \"nsfw\" : \"not safe for work\",\n",
    "    \"nth\" : \"nice to have\",\n",
    "    \"nvr\" : \"never\",\n",
    "    \"nyc\" : \"new york city\",\n",
    "    \"oc\" : \"original content\",\n",
    "    \"og\" : \"original\",\n",
    "    \"ohp\" : \"overhead projector\",\n",
    "    \"oic\" : \"oh i see\",\n",
    "    \"omdb\" : \"over my dead body\",\n",
    "    \"omg\" : \"oh my god\",\n",
    "    \"omw\" : \"on my way\",\n",
    "    \"p.a\" : \"per annum\",\n",
    "    \"p.m\" : \"after midday\",\n",
    "    \"pm\" : \"prime minister\",\n",
    "    \"poc\" : \"people of color\",\n",
    "    \"pov\" : \"point of view\",\n",
    "    \"pp\" : \"pages\",\n",
    "    \"ppl\" : \"people\",\n",
    "    \"prw\" : \"parents are watching\",\n",
    "    \"ps\" : \"postscript\",\n",
    "    \"pt\" : \"point\",\n",
    "    \"ptb\" : \"please text back\",\n",
    "    \"pto\" : \"please turn over\",\n",
    "    \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "    \"ratchet\" : \"rude\",\n",
    "    \"rbtl\" : \"read between the lines\",\n",
    "    \"rlrt\" : \"real life retweet\", \n",
    "    \"rofl\" : \"rolling on the floor laughing\",\n",
    "    \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\" : \"retweet\",\n",
    "    \"ruok\" : \"are you ok\",\n",
    "    \"sfw\" : \"safe for work\",\n",
    "    \"sk8\" : \"skate\",\n",
    "    \"smh\" : \"shake my head\",\n",
    "    \"sq\" : \"square\",\n",
    "    \"srsly\" : \"seriously\", \n",
    "    \"ssdd\" : \"same stuff different day\",\n",
    "    \"tbh\" : \"to be honest\",\n",
    "    \"tbs\" : \"tablespooful\",\n",
    "    \"tbsp\" : \"tablespooful\",\n",
    "    \"tfw\" : \"that feeling when\",\n",
    "    \"thks\" : \"thank you\",\n",
    "    \"tho\" : \"though\",\n",
    "    \"thx\" : \"thank you\",\n",
    "    \"tia\" : \"thanks in advance\",\n",
    "    \"til\" : \"today i learned\",\n",
    "    \"tl;dr\" : \"too long i did not read\",\n",
    "    \"tldr\" : \"too long i did not read\",\n",
    "    \"tmb\" : \"tweet me back\",\n",
    "    \"tntl\" : \"trying not to laugh\",\n",
    "    \"ttyl\" : \"talk to you later\",\n",
    "    \"u\" : \"you\",\n",
    "    \"u2\" : \"you too\",\n",
    "    \"u4e\" : \"yours for ever\",\n",
    "    \"utc\" : \"coordinated universal time\",\n",
    "    \"w/\" : \"with\",\n",
    "    \"w/o\" : \"without\",\n",
    "    \"w8\" : \"wait\",\n",
    "    \"wassup\" : \"what is up\",\n",
    "    \"wb\" : \"welcome back\",\n",
    "    \"wtf\" : \"what the fuck\",\n",
    "    \"wtg\" : \"way to go\",\n",
    "    \"wtpa\" : \"where the party at\",\n",
    "    \"wuf\" : \"where are you from\",\n",
    "    \"wuzup\" : \"what is up\",\n",
    "    \"wywh\" : \"wish you were here\",\n",
    "    \"yd\" : \"yard\",\n",
    "    \"ygtr\" : \"you got that right\",\n",
    "    \"ynk\" : \"you never know\",\n",
    "    \"zzz\" : \"sleeping bored and tired\"\n",
    "}\n",
    "\n",
    "\n",
    "def reinsert_tags(text: str, tag_split: str, tag_prefix: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Given a certain text, look for a separator that has been used for tag splitting\n",
    "    and make sure that all tags are put between []\n",
    "    \n",
    "    :param text: text to be adjusted\n",
    "    :param tag_split: tag to be splitted\n",
    "    :param tag_prefix: prefix to apply on tagging\n",
    "    :return: tagged text\n",
    "    \"\"\"\n",
    "    open_tag = False\n",
    "    full_sentence = \"\"\n",
    "    for sentence in text.split(tag_split):\n",
    "        if open_tag:\n",
    "            full_sentence += f\" [{tag_prefix}{sentence.upper().strip()}] \"\n",
    "        else:\n",
    "            full_sentence += sentence\n",
    "        open_tag = not open_tag\n",
    "\n",
    "    full_sentence = re.sub(\" +\", \" \", full_sentence)\n",
    "    return full_sentence.strip()\n",
    "\n",
    "\n",
    "def clean_content(\n",
    "    text, \n",
    "    fix_contraction: bool = False, \n",
    "    tag_emoji: bool = False,\n",
    "    tagged_items: list = [\"NAME\", \"RELIGION\"],\n",
    "    handles: bool = False, \n",
    "    case: bool = False,\n",
    "    links: bool = False,\n",
    "    non_char: bool = False,\n",
    "    rm_stop_words: bool = False,\n",
    "    adjust_slangs: bool = False,\n",
    "    lemmatization: bool = False,\n",
    "    stemming: bool = True,\n",
    "    tokenize: bool = False,\n",
    "    ponctuation: bool = False,\n",
    "    unmapped_emoji: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Apply data cleaning to text given a range of options\n",
    "    \n",
    "    :param fix_contraction: True if we want to remove ABBREVIATIONS\n",
    "    :param tag_emoji: True if we want to tag emojis by name\n",
    "    :param tagged_items: List of items that are tagged in the current text\n",
    "    :param handles: True if we want to remove twitter like handles\n",
    "    :param case: True if we want to normalize to lower case\n",
    "    :param links: True if we want to remove websites and links\n",
    "    :param non_char: True if we want to remove non-character words\n",
    "    :param rm_stop_words: True if we want to remove stop words\n",
    "    :param lemmatization: True if we want to apply lemmatization\n",
    "    :param tokenize: True if we want to apply twitter tokenization\n",
    "    :param ponctuation: True if we want to remove ponctuation\n",
    "    :param unmapped_emoji: True if we want to remove unmapped emojis\n",
    "    :return: clean text\n",
    "    \"\"\"\n",
    "    clean_text = text\n",
    "    \n",
    "    # replaces ABBREVIATIONS with full word versions\n",
    "    if fix_contraction:\n",
    "        clean_text = contractions.fix(text)\n",
    "    \n",
    "    # replaces emojis\n",
    "    if tag_emoji:\n",
    "        clean_text = \"\".join(\n",
    "            [c if c not in emoji.EMOJI_DATA else emoji.EMOJI_DATA[c][\"en\"].replace(\":\", \" _emoji_ \") for c in clean_text]\n",
    "        )\n",
    "    \n",
    "    # remove reddit handles\n",
    "    if handles:\n",
    "        clean_text = re.sub(r\"@\\w+\\s?\", \"\", clean_text)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    if case:\n",
    "        clean_text = clean_text.lower()\n",
    "    \n",
    "\n",
    "    if links:\n",
    "         # remove links http:// or https://\n",
    "        clean_text = re.sub(r\"https?:\\/\\/\\S+\", \"\", clean_text)\n",
    "    \n",
    "        # remove links beginning with www. and ending with .com\n",
    "        clean_text = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", \"\", clean_text)\n",
    "    \n",
    "        # remove html reference characters\n",
    "        clean_text = re.sub(r\"&[a-z]+;\", \"\", clean_text)\n",
    "        \n",
    "    # deal with tagged items\n",
    "    for tag in tagged_items:\n",
    "        clean_text = re.sub(fr\"\\[{tag}\\]\", f\" _tag_ {tag} _tag_ \", clean_text)  \n",
    "    \n",
    "    # remove non-letter characters besides spaces \"/\", \";\" \"[\", \"]\" \"=\", \"#\"\n",
    "    if non_char:\n",
    "        clean_text = re.sub(r\"[/;\\[\\]=#]\", \"\", clean_text)  \n",
    "    \n",
    "    # remove stop words\n",
    "    if rm_stop_words:\n",
    "        clean_lst = []\n",
    "        for word in clean_text.split():\n",
    "            if word not in STOP_WORDS:\n",
    "                clean_lst.append(word)\n",
    "    else:\n",
    "        clean_lst = clean_text.split()\n",
    "    \n",
    "    \n",
    "    # adjust slangs\n",
    "    if adjust_slangs:\n",
    "        clean_lst = [ABBREVIATIONS.get(word.lower(), word) for word in clean_lst]   \n",
    "        \n",
    "    # apply lemmatization\n",
    "    if lemmatization:\n",
    "        lemmatized_words = []\n",
    "        for word in clean_lst:\n",
    "            # lemmatized_word = LEMMATIZER.lemmatize(word)\n",
    "            lemmatized_word = Word(word).lemmatize()\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "        clean_lst = lemmatized_words\n",
    "    \n",
    "    if stemming:\n",
    "        stemmed_words = []\n",
    "        for word in clean_lst:\n",
    "            stemmed_word = STEEMER.stem(word)\n",
    "            stemmed_words.append(stemmed_word)\n",
    "        clean_lst = stemmed_words\n",
    "    \n",
    "    # concatenate the text again\n",
    "    clean_text = \" \".join(clean_lst)\n",
    "    \n",
    "    # apply tokenization\n",
    "    if tokenize:\n",
    "        tokens = TOKENIZER.tokenize(clean_text)\n",
    "    else:\n",
    "        tokens = clean_text.split(\" \")\n",
    "    \n",
    "    if ponctuation:\n",
    "        clean_text = \" \".join([w for w in tokens if w not in PUNCUATION_LIST])\n",
    "    else:\n",
    "        clean_text = \" \".join(tokens)\n",
    "        \n",
    "    # clean emojis that were not mapped by the library\n",
    "    if unmapped_emoji:\n",
    "        clean_text = \"\".join([w for w in clean_text if ord(w) < 2000])\n",
    "        \n",
    "    # add the tags for emojis\n",
    "    if tag_emoji:\n",
    "        clean_text = reinsert_tags(clean_text, \"_emoji_\", \"EMOJI_\")\n",
    "    \n",
    "    # re-insert tags\n",
    "    if len(tagged_items) > 0:\n",
    "        clean_text = reinsert_tags(clean_text, \"_tag_\")\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a86fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.209658Z",
     "start_time": "2023-08-01T21:30:43.186548Z"
    }
   },
   "outputs": [],
   "source": [
    "text = dataset.text.sample(1).values[0]\n",
    "print(text)\n",
    "print(clean_content(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515073d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.225704Z",
     "start_time": "2023-08-01T21:30:43.212620Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content(\"No. Or at least thatâ€™s what I suspectðŸ¤”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05c3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.241731Z",
     "start_time": "2023-08-01T21:30:43.226680Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_content(\n",
    "    \"No. Or at least thatâ€™s what I suspectðŸ¤”\", \n",
    "    fix_contraction=False, \n",
    "    tag_emoji=True,\n",
    "    tagged_items=[\"NAME\", \"RELIGION\"],\n",
    "    handles=False, \n",
    "    case=False,\n",
    "    links=False,\n",
    "    non_char=False,\n",
    "    rm_stop_words=False,\n",
    "    lemmatization=False,\n",
    "    tokenize=False,\n",
    "    ponctuation=False,\n",
    "    unmapped_emoji=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea0056",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8608bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.520711Z",
     "start_time": "2023-08-01T21:30:43.243712Z"
    }
   },
   "outputs": [],
   "source": [
    "EMOJIS_FOUND = (\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            (c,) \n",
    "            for text in dataset[\"text\"].to_list()\n",
    "            for c in text\n",
    "            if c in emoji.EMOJI_DATA \n",
    "        ],\n",
    "        columns=[\"emoji\"]\n",
    "    )\n",
    "    .assign(count=1)\n",
    "    .groupby([\"emoji\"], as_index=False)[\"count\"]\n",
    "    .sum()\n",
    "    .assign(pct=lambda f: f[\"count\"] / f[\"count\"].sum())\n",
    "    .assign(cum_pct=lambda f: f[\"pct\"].cumsum())\n",
    ")\n",
    "\n",
    "\n",
    "def apply_tokenization(\n",
    "    t_model: TFAutoModel, \n",
    "    tokenizer: AutoTokenizer, \n",
    "    data: pd.DataFrame, \n",
    "    emoji_tagging: bool = False, \n",
    "    emoji_threshold: float = 0.8\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Given a tokenizer object and some data, apply the tokenization process on the\n",
    "    train, test and validation sets and generate a dictionary of processed data with\n",
    "    all inputs needed for the model\n",
    "    \n",
    "    :param t_model: transformer model\n",
    "    :param tokenizer: tokenizer object\n",
    "    :param data: input data to be used for training\n",
    "    :param emoji_tagging: flag if we should apply emoji tagging\n",
    "    :param emoji_threshold: threshold for selection of emojis to be added\n",
    "    \"\"\"\n",
    "    processed = dict()\n",
    "    \n",
    "    # select words to be added\n",
    "    words_to_add = [\"[NAME]\", \"[RELIGION]\"]\n",
    "    if emoji_tagging:\n",
    "        words_to_add += EMOJIS_FOUND.loc[lambda f: f[\"cum_pct\"] <= emoji_threshold].emoji.to_list()\n",
    "        words_to_add += [\":)\", \":(\", \"XD\", \"xD\", \":D\", \":'(\"]\n",
    "        \n",
    "    # add the new tokens\n",
    "    tokenizer.add_tokens(words_to_add)\n",
    "    t_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # for each group of data\n",
    "    for group in tqdm([\"train\", \"validation\", \"test\"]):\n",
    "        # pivot the adtaset and extract the emotions\n",
    "        df = data.loc[lambda f: f[\"set\"] == group].pivot_table(\n",
    "            index=[\"code\", \"text\"],\n",
    "            columns=[\"goemotion\"],\n",
    "            values=\"set\",\n",
    "            aggfunc=\"count\",\n",
    "            fill_value=0\n",
    "        ).reset_index().drop(columns=[\"none\"], errors=\"ignore\")\n",
    "\n",
    "        processed[group] = dict()\n",
    "        \n",
    "        # apply the tokenizer to the data\n",
    "        processed[group][\"tokens\"] = tokenizer(\n",
    "            df.text.to_list(), \n",
    "            max_length=MAX_SEQUENCE_LENGTH, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        \n",
    "        # create same inputs to be used in the model\n",
    "        processed[group][\"inputs\"] = [\n",
    "            processed[group][\"tokens\"].input_ids, \n",
    "            processed[group][\"tokens\"].token_type_ids, \n",
    "            processed[group][\"tokens\"].attention_mask\n",
    "        ]\n",
    "        \n",
    "        # create the set of labels\n",
    "        processed[group][\"labels\"] = df.iloc[:, 2:].values\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4763186",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939daff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.536711Z",
     "start_time": "2023-08-01T21:30:43.522713Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_cls_model(\n",
    "    t_model: TFAutoModel,\n",
    "    trainable: str = \"all\",\n",
    "    head: str = \"none\",\n",
    "    dropout: float = 0.3,\n",
    "    label_smoothing: float = 0.1,\n",
    "    hidden_size: int = 256, \n",
    "    hidden_layers: list = [256, 128, 64],\n",
    "    dropout_layers: list = [0.3, 0.3, 0.3],\n",
    "    num_filters: list = [100, 100, 50, 25],\n",
    "    kernel_sizes: list = [2, 3, 4, 5],\n",
    "    learning_rate: float = 0.00005,\n",
    "    epsilon: float = 1e-08,\n",
    "    num_classes: int = 28,\n",
    "    loss_type: str = \"normal\",\n",
    "    max_sequence_length: int = MAX_SEQUENCE_LENGTH,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a classification model using a pre-trained transformer\n",
    "    \n",
    "    :param t_model: transformer model\n",
    "    :param trainable: select whith parts of the transformer model are trainable (all, last, none)\n",
    "    :param head: type of head to be applied (none, dense, mlp, cnn)\n",
    "    :param dropout: dropout value to be selected\n",
    "    :param label_smoothing: label smoothing to be applied\n",
    "    :param hidden_size: number of nodes for head=dense\n",
    "    :param hidden_layers: number of nodes per layer for head=mlp\n",
    "    :param dropout_layers: dropout rate for each hidden layer for head=mlp\n",
    "    :param num_filters: number of filters to use for head=cnn\n",
    "    :param kernel_sizes: kernal sizes to use for head=cnn\n",
    "    :param learning_rate: learning rate applied for Adam\n",
    "    :param epsilon: epsilon selected for Adam\n",
    "    :param num_classes: number of classes to predict\n",
    "    :param max_sequence_length: maximum sequence length selected\n",
    "    \"\"\"\n",
    "    # set the model to be trainable\n",
    "    if trainable == \"all\":\n",
    "        t_model.trainable = True\n",
    "    elif trainable == \"last\":\n",
    "        last_layer_num = max(\n",
    "            [\n",
    "                int(w.name[w.name.index(\"layer_._\"):].split(\"/\")[0].replace(\"layer_._\", \"\"))\n",
    "                for w in t_model.weights if \"layer_._\" in w.name\n",
    "            ]\n",
    "        )\n",
    "        for w in t_model.weights:\n",
    "            if f\"layer_._{last_layer_num}\" not in w.name:\n",
    "                w._trainable = False\n",
    "    elif trainable == \"none\":\n",
    "        t_model.trainable = False\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # extract input ids, token ids and the attention mask\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name=\"input_ids_layer\")\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name=\"token_type_ids_layer\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name=\"attention_mask_layer\")\n",
    "    \n",
    "    # encode this into the model\n",
    "    model_inputs = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}      \n",
    "    output = t_model(model_inputs)\n",
    "    \n",
    "    # if no head was selected, pass the pooler token to a dropout layer\n",
    "    if head == \"none\":\n",
    "        pooler_token = output[1]\n",
    "        hidden = tf.keras.layers.Dropout(dropout)(pooler_token)\n",
    "        \n",
    "    # if a dense head was selected, add a hidden layer with the selected hidden size\n",
    "    elif head == \"dense\":\n",
    "        pooler_token = output[1]\n",
    "        hidden = tf.keras.layers.Dense(hidden_size, activation=\"relu\", name=\"hidden_layer\")(pooler_token)\n",
    "        hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
    "    \n",
    "    # if multi-layer perceptron was selected, add the hidden layers on top of the pooler token\n",
    "    elif head == \"mlp\":\n",
    "        pooler_token = output[1]\n",
    "        hidden = tf.keras.layers.Dense(hidden_layers[0], activation=\"relu\", name=\"hidden_layer\")(pooler_token)\n",
    "        i = 0\n",
    "        for size in hidden_layers[1:]:\n",
    "            hidden = tf.keras.layers.Dropout(dropout_layers[i])(hidden)\n",
    "            hidden = tf.keras.layers.Dense(size, activation=\"relu\", name=\"hidden_layer\")(hidden)\n",
    "            i += 1\n",
    "        hidden = tf.keras.layers.Dropout(dropout_layers[-1])(hidden)\n",
    "    \n",
    "    # if cnn was selected, get the token embeddings and create a cnn network\n",
    "    elif head == \"cnn\":\n",
    "        token_embeddings = output[0]\n",
    "        cnn_outputs = []\n",
    "        for filters, kernel_size in zip(num_filters, kernel_sizes):\n",
    "            conv_layer = tf.keras.layers.Conv1D(\n",
    "                filters=filters, kernel_size=kernel_size, activation='relu'\n",
    "            )(token_embeddings)\n",
    "            max_pool = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "            cnn_outputs.append(max_pool)\n",
    "        cnn_concat = tf.keras.layers.concatenate(cnn_outputs)\n",
    "        \n",
    "        hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cnn_concat)\n",
    "        hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
    "    \n",
    "    # with the final hidden layer, add a dense layer for the classification task\n",
    "    classification = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"classification_layer\")(hidden)\n",
    "    \n",
    "    # instantiate the classification model\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
    "    \n",
    "    # compile using the learning rate and selected epsilon\n",
    "    if loss_type == \"normal\":\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing)\n",
    "    elif loss_type == \"focal\":\n",
    "        loss = tf.keras.losses.CategoricalFocalCrossentropy(alpha=0.6, gamma=2.0, label_smoothing=label_smoothing)\n",
    "    \n",
    "    classification_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon),\n",
    "        loss=loss, \n",
    "        metrics=[tf.keras.metrics.F1Score(average=\"macro\", threshold=0.2)]\n",
    "    )\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc372e38",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e9edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.551742Z",
     "start_time": "2023-08-01T21:30:43.537714Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "TARGET_NAMES = sorted(list(dataset[\"goemotion\"].unique()))\n",
    "\n",
    "\n",
    "def evaluate_model(p_set: str, proba: pd.DataFrame, dataset: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the model results\n",
    "    \n",
    "    :param p_set: which prediction set to use (test, validation)\n",
    "    :param dataset: the complete dataset\n",
    "    :return: model evaluation values\n",
    "    \"\"\"\n",
    "    outputs = dict()\n",
    "    df = dataset.loc[lambda f: f[\"set\"] == p_set]\n",
    "    \n",
    "    # get the one-hot-encoded values\n",
    "    pv = df.pivot_table(index=[\"code\"], columns=\"goemotion\", values=\"set\", aggfunc=\"count\", fill_value=0)\n",
    "    \n",
    "    # generate the predictions\n",
    "    proba = proba.loc[pv.index, pv.columns]\n",
    "    predictions = (proba.values > 0.2).astype(int)\n",
    "    pred = (\n",
    "        proba.reset_index()\n",
    "        .rename(columns={\"index\": \"code\"})\n",
    "        .melt(id_vars=[\"code\"], var_name=\"goemotion\", value_name=\"proba\")\n",
    "    )\n",
    "    pred[\"flag\"] = pred[\"proba\"] > 0.2\n",
    "    outputs[\"predictions\"] = pred\n",
    "    \n",
    "    # calculate metrics\n",
    "    outputs[\"f1_macro\"] = f1_score(pv[TARGET_NAMES].values, predictions, average=\"macro\")\n",
    "    outputs[\"f1_micro\"] = f1_score(pv[TARGET_NAMES].values, predictions, average=\"micro\")\n",
    "    outputs[\"roc_auc\"] = roc_auc_score(pv[TARGET_NAMES].values, proba.values, average=\"macro\", multi_class=\"ovo\")\n",
    "    outputs[\"confusion_matrix\"] = confusion_matrix(\n",
    "        np.argmax(pv[TARGET_NAMES].values, axis=1), np.argmax(predictions, axis=1)\n",
    "    )\n",
    "    outputs[\"classification_report\"] = classification_report(\n",
    "        pv[TARGET_NAMES].values, predictions, target_names=TARGET_NAMES\n",
    "    )\n",
    "    \n",
    "    # get the misclassification value\n",
    "    df = df.merge(pred.loc[lambda f: f[\"flag\"] == 1], on=[\"code\", \"goemotion\"], how=\"left\")\n",
    "    corrclass = df[df[\"flag\"].notnull()]\n",
    "    misclass = df[df[\"flag\"].isnull()]\n",
    "    outputs[\"misclassification\"] = misclass\n",
    "    \n",
    "    # get misclassification examples\n",
    "    outputs[\"misclassification_examples\"] = {\n",
    "        label: misclass[misclass[\"goemotion\"] == label]\n",
    "        .sample(3, replace=True)\n",
    "        .drop_duplicates()\n",
    "        .text\n",
    "        .to_list()\n",
    "        for label in TARGET_NAMES\n",
    "        if misclass[misclass[\"goemotion\"] == label].shape[0] > 0\n",
    "    }\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9238bf",
   "metadata": {},
   "source": [
    "### Scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fd396",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.566711Z",
     "start_time": "2023-08-01T21:30:43.552713Z"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler_10(epoch, lr):\n",
    "    return lr\n",
    "\n",
    "\n",
    "def scheduler_05(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "    \n",
    "def scheduler_02(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * 0.2\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "\n",
    "def scheduler_01(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * 0.1\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "    \n",
    "def scheduler_exp(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638956db",
   "metadata": {},
   "source": [
    "### Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c214d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.630711Z",
     "start_time": "2023-08-01T21:30:43.567712Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import ipynbname\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "NB_FNAME = ipynbname.name()\n",
    "\n",
    "SCHEDULERS = {\"1.0\": scheduler_10, \"0.5\": scheduler_05, \"0.2\": scheduler_02, \"0.1\": scheduler_01, \"exp\": scheduler_exp}\n",
    "\n",
    "\n",
    "def limit_mem():\n",
    "    tf.compat.v1.keras.backend.get_session().close()\n",
    "    cfg = tf.compat.v1.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=cfg))\n",
    "    \n",
    "\n",
    "\n",
    "def run_model_experiment(grid: dict, dataset: pd.DataFrame):   \n",
    "    # select the appropriate dataset based on the balancing parameter\n",
    "    if grid[\"balancing\"] != \"none\":\n",
    "        if isinstance(grid[\"balancing\"], list):\n",
    "            data_for_exp = apply_balancing(dataset, \"avg\", False)\n",
    "            data_for_exp = data_for_exp.loc[\n",
    "                lambda f: (f[\"set\"] != \"train\") | (\n",
    "                    (f[\"set\"] == \"train\") & (~f[\"goemotion\"].isin(grid[\"balancing\"])) & (~f[\"for_balance\"])\n",
    "                ) | (\n",
    "                    (f[\"set\"] == \"train\") & (f[\"goemotion\"].isin(grid[\"balancing\"]))\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            data_for_exp = apply_balancing(dataset, grid[\"balancing\"], grid[\"augment\"])\n",
    "    else:\n",
    "        data_for_exp = dataset.copy()\n",
    "\n",
    "    # select the combinations to execute\n",
    "    combinations = {k: (k,) for k in data_for_exp.goemotion.unique()}\n",
    "    if len(grid[\"minority_shuffling\"]) > 0:\n",
    "        combinations = grid[\"minority_shuffling\"]\n",
    "\n",
    "    # apply cleaning to the dataset\n",
    "    if grid[\"clean_data\"]:\n",
    "        data_for_exp.text = data_for_exp.text.apply(clean_content)\n",
    "\n",
    "    # get the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(grid[\"model\"])\n",
    "    try:\n",
    "        t_model = TFAutoModel.from_pretrained(grid[\"model\"])\n",
    "    except OSError:\n",
    "        t_model = TFAutoModel.from_pretrained(grid[\"model\"], from_pt=True)\n",
    "\n",
    "    # get how many model replacements we should make\n",
    "    replaces = [{v: k for k, vl in combinations.items() for v in vl}]\n",
    "    for k, vl in combinations.items():\n",
    "        if len(vl) > 1:\n",
    "            replaces.append({v: v for v in vl})\n",
    "            \n",
    "    # create the folder\n",
    "    folder = Path(\"../experiments/\" + pd.to_datetime(\"today\").strftime(\"%Y%m%dT%H%M%S\"))\n",
    "    folder.mkdir(exist_ok=False, parents=True)\n",
    "\n",
    "    # run the model loop\n",
    "    predictions = dict()\n",
    "    for i, comb in enumerate(replaces):\n",
    "        # select the data to be used for training\n",
    "        data_for_training = (\n",
    "            data_for_exp.loc[lambda f: ((f[\"goemotion\"].isin(comb)) & (f[\"set\"] == \"train\")) | (f[\"set\"] != \"train\")]\n",
    "            .assign(goemotion=lambda f: f[\"goemotion\"].apply(lambda x: \"none\" if x not in comb else x))\n",
    "            .assign(goemotion=lambda f: f[\"goemotion\"].replace(comb))\n",
    "        )\n",
    "\n",
    "        # apply tokenization\n",
    "        processed = apply_tokenization(t_model, tokenizer, data_for_training, grid[\"emoji_tagging\"])\n",
    "\n",
    "        # create the model\n",
    "        cls_model = create_cls_model(\n",
    "            t_model, \n",
    "            trainable=grid[\"trainable\"], \n",
    "            head=grid[\"head\"], \n",
    "            dropout=grid.get(\"dropout\", 0.3),\n",
    "            label_smoothing=grid[\"label_smoothing\"],\n",
    "            learning_rate=grid[\"learning_rate\"],\n",
    "            num_classes=data_for_training.loc[lambda f: f[\"set\"] == \"train\"].goemotion.nunique(),\n",
    "            loss_type=grid.get(\"loss\", \"focal\"),\n",
    "        )\n",
    "        \n",
    "        # get the indexes that make the validation set\n",
    "        indexes = (\n",
    "            data_for_training.loc[lambda f: f[\"set\"] == \"validation\"]\n",
    "            .pivot_table(index=\"code\", columns=[\"goemotion\"], values=\"set\", aggfunc=\"count\", fill_value=0)\n",
    "            .drop(columns=[\"none\"], errors=\"ignore\")\n",
    "            .sum(axis=1)\n",
    "            .reset_index()\n",
    "            .loc[lambda f: f[0] > 0]\n",
    "            .index\n",
    "        )\n",
    "        val_inputs = [tf.convert_to_tensor(tensor.numpy()[indexes]) for tensor in processed[\"validation\"][\"inputs\"]]\n",
    "        val_labels = processed[\"validation\"][\"labels\"][indexes]\n",
    "\n",
    "        # fit the model\n",
    "        cb_scheduler = tf.keras.callbacks.LearningRateScheduler(SCHEDULERS[grid[\"scheduler\"]])\n",
    "        cb_earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model_history = cls_model.fit(\n",
    "            processed[\"train\"][\"inputs\"],\n",
    "            processed[\"train\"][\"labels\"].astype(float),\n",
    "            validation_data=(val_inputs, val_labels.astype(float)),\n",
    "            batch_size=grid[\"batch_size\"],\n",
    "            epochs=grid[\"epochs\"],\n",
    "            callbacks=[cb_scheduler, cb_earlystop],\n",
    "        )\n",
    "\n",
    "        cls_model.save_weights(folder / f'checkpoint_{i}')\n",
    "        \n",
    "        # save the history data\n",
    "        with open(folder / f\"history_{i}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(model_history.history, f)\n",
    "\n",
    "        # run the probability calculation\n",
    "        classes = sorted(list(set(comb.values())))\n",
    "        predictions[tuple(classes)] = dict()\n",
    "        for p_set in [\"validation\", \"test\"]:\n",
    "            index = (\n",
    "                data_for_training.loc[lambda f: f[\"set\"] == p_set]\n",
    "                .pivot_table(index=\"code\", columns=[\"goemotion\"], values=\"set\", aggfunc=\"count\", fill_value=0)\n",
    "                .index\n",
    "            )\n",
    "            predictions[tuple(classes)][p_set] = pd.DataFrame(\n",
    "                cls_model.predict(processed[p_set][\"inputs\"]), index=index, columns=classes,\n",
    "            )\n",
    "    \n",
    "    # export the base predictions\n",
    "    with open(folder / \"original_predictions.pkl\", \"wb\") as f:\n",
    "        pickle.dump(predictions, f)\n",
    "    \n",
    "    # ensure for the post predictions that we have the expected values\n",
    "    base = predictions[list(predictions)[0]]\n",
    "    for k, vl in combinations.items():\n",
    "        if len(vl) > 1:\n",
    "            values = predictions[tuple(sorted(vl))]\n",
    "            for p_set in [\"validation\", \"test\"]:\n",
    "                base[p_set] = pd.concat(\n",
    "                    [base[p_set].drop(columns=k), values[p_set].multiply(base[p_set][k], axis=0)], axis=1\n",
    "                )\n",
    "\n",
    "    # run the model evaluation on validation\n",
    "    val_res = evaluate_model(\"validation\", base[\"validation\"], data_for_exp)\n",
    "    test_res = evaluate_model(\"test\", base[\"test\"], data_for_exp)\n",
    "    \n",
    "    # save the model val_res\n",
    "    with open(folder / \"grid_params.json\", \"w\") as f:\n",
    "        json.dump(grid, f)\n",
    "    \n",
    "    for res, name in [(val_res, \"validation\"), (test_res, \"test\")]:\n",
    "        with open(folder / f\"metrics_{name}.json\", \"w\") as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    r: v \n",
    "                    for r, v in res.items() \n",
    "                    if r not in [\"confusion_matrix\", \"predictions\", \"misclassification\"]\n",
    "                }, \n",
    "                f\n",
    "            )\n",
    "        with open(folder / f\"confusion_matrix_{name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(res[\"confusion_matrix\"], f)\n",
    "\n",
    "        res[\"predictions\"].to_pickle(folder / f\"prediction_{name}.pkl\")\n",
    "        res[\"misclassification\"].to_pickle(folder / f\"misclassification_{name}.pkl\")\n",
    "    \n",
    "    data_for_exp.to_pickle(folder / \"data_for_exp.pkl\")\n",
    "    \n",
    "    shutil.copyfile(os.path.abspath(f\"{NB_FNAME}.ipynb\"), folder / f\"{NB_FNAME}.ipynb\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9f20267-93f8-4cea-b951-8b8129112d2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T03:34:54.097275Z",
     "start_time": "2023-08-01T03:33:31.441467Z"
    },
    "scrolled": true
   },
   "source": [
    "np.random.seed(33)\n",
    "tf.random.set_seed(33)\n",
    "\n",
    "grid = {\n",
    "    \"model\": \"bert-base-uncased\",\n",
    "    \"trainable\": \"last\",\n",
    "    \"head\": \"none\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"scheduler\": \"1.0\",\n",
    "    \"dropout\": 0.3,\n",
    "    \"label_smoothing\": 0.05,\n",
    "    \"learning_rate\": 0.00005,\n",
    "    \"emoji_tagging\": True,\n",
    "    \"clean_data\": True,\n",
    "    \"balancing\": \"none\",\n",
    "    \"augment\": False,\n",
    "    \"minority_shuffling\": {},\n",
    "}\n",
    "\n",
    "limit_mem()\n",
    "tf.keras.backend.clear_session()\n",
    "while gc.collect():\n",
    "    continue\n",
    "run_model_experiment(grid, dataset.sample(frac=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f321ad8b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6818e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T21:30:43.676717Z",
     "start_time": "2023-08-01T21:30:43.662713Z"
    }
   },
   "outputs": [],
   "source": [
    "MINORITY_SHUFFLING_1 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"relief_joy\": (\"relief\", \"joy\"),\n",
    "    \"anger_annoyance\": (\"anger\", \"annoyance\"),\n",
    "    \"nervousness\": (\"nervousness\",),\n",
    "    \"fear\": (\"fear\", ),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"neutral\": (\"neutral\",),\n",
    "    \"optimism\": (\"optimism\",),\n",
    "    \"desire\": (\"desire\",),\n",
    "    \"love\": (\"love\",),\n",
    "    \"disapproval\": (\"disapproval\",),\n",
    "    \"amusement\": (\"amusement\",),\n",
    "    \"caring\": (\"caring\",),\n",
    "    \"excitement\": (\"excitement\",),\n",
    "    \"curiosity\": (\"curiosity\",),\n",
    "    \"embarrassment\": (\"embarrassment\",),\n",
    "    \"disgust\": (\"disgust\",),\n",
    "    \"gratitude\": (\"gratitude\",),\n",
    "    \"confusion\": (\"confusion\",),\n",
    "    \"disappointment\": (\"disappointment\",),\n",
    "    \"remorse\": (\"remorse\",)\n",
    "}\n",
    "\n",
    "MINORITY_SHUFFLING_2 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"relief_approval\": (\"relief\", \"approval\"),\n",
    "    \"anger_annoyance\": (\"anger\", \"annoyance\"),\n",
    "    \"nervousness_fear\": (\"nervousness\", \"fear\"),\n",
    "    \"joy\": (\"joy\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"neutral\": (\"neutral\",),\n",
    "    \"optimism\": (\"optimism\",),\n",
    "    \"desire\": (\"desire\",),\n",
    "    \"love\": (\"love\",),\n",
    "    \"disapproval\": (\"disapproval\",),\n",
    "    \"amusement\": (\"amusement\",),\n",
    "    \"caring\": (\"caring\",),\n",
    "    \"excitement\": (\"excitement\",),\n",
    "    \"curiosity\": (\"curiosity\",),\n",
    "    \"embarrassment\": (\"embarrassment\",),\n",
    "    \"disgust\": (\"disgust\",),\n",
    "    \"gratitude\": (\"gratitude\",),\n",
    "    \"confusion\": (\"confusion\",),\n",
    "    \"disappointment\": (\"disappointment\",),\n",
    "    \"remorse\": (\"remorse\",)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494346e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a3019",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9429dee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T01:25:49.586988Z",
     "start_time": "2023-08-02T00:02:48.962727Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = \"42\"\n",
    "\n",
    "for grid in [\n",
    "    {\n",
    "        \"model\": \"bert-base-uncased\",\n",
    "        \"trainable\": \"all\",\n",
    "        \"head\": \"none\",\n",
    "        \"scheduler\": \"0.2\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"label_smoothing\": 0,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"emoji_tagging\": True,\n",
    "        \"clean_data\": False,\n",
    "        \"balancing\": \"none\",\n",
    "        \"augment\": False,\n",
    "        \"minority_shuffling\": MINORITY_SHUFFLING_1,\n",
    "        \"loss\": \"normal\",\n",
    "    },\n",
    "]:\n",
    "    limit_mem()\n",
    "    tf.keras.backend.clear_session()\n",
    "    while gc.collect():\n",
    "        continue\n",
    "    run_model_experiment(grid, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd15b4",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
