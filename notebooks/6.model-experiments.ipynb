{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205a9e54",
   "metadata": {},
   "source": [
    "# Balancing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5e36e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4782f5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54e281e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:49.926079Z",
     "start_time": "2023-07-31T19:58:42.796059Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import torch\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c412bec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:49.942046Z",
     "start_time": "2023-07-31T19:58:49.928046Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e17b6bb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:49.958045Z",
     "start_time": "2023-07-31T19:58:49.943047Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96db02cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:50.005510Z",
     "start_time": "2023-07-31T19:58:49.960047Z"
    }
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682e6e9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:50.021504Z",
     "start_time": "2023-07-31T19:58:50.006507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0aca1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e55a9e",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c392e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:50.164846Z",
     "start_time": "2023-07-31T19:58:50.022505Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_parquet(\"../data/clean_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a5a9f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71775a39",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762431da",
   "metadata": {},
   "source": [
    "### Balancing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037e2ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:51.333925Z",
     "start_time": "2023-07-31T19:58:50.165710Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.translate import NotTranslated\n",
    "\n",
    "\n",
    "SR = random.SystemRandom()\n",
    "LANGUAGES = [\"es\", \"de\", \"fr\", \"ar\", \"te\", \"hi\", \"ja\", \"fa\", \"sq\", \"bg\", \"nl\", \"gu\", \"ig\", \"kk\", \"mt\", \"ps\"]\n",
    "TRANSLATIONS = dict()\n",
    "\n",
    "\n",
    "def data_augmentation(message: str, language: str = \"en\", aug_range: int = 1) -> list:\n",
    "    \"\"\"\n",
    "    Create new text data by translating a message to a random language\n",
    "    and then tranlating it back\n",
    "    \n",
    "    :param message: messege to be translated\n",
    "    :param language: original language of the message\n",
    "    :param aug_range: number of new messages to generate\n",
    "    :return: list of new messages\n",
    "    \"\"\"\n",
    "    augmented_messages = []\n",
    "    if hasattr(message, \"decode\"):\n",
    "        message = message.decode(\"utf-8\")\n",
    "\n",
    "    for j in range(0, aug_range) :\n",
    "        text_blob = TextBlob(message)\n",
    "        try:\n",
    "            to_lang = SR.choice(LANGUAGES)\n",
    "            \n",
    "            if (message, to_lang) in TRANSLATIONS:\n",
    "                text_blob = TRANSLATIONS[(message, to_lang)]\n",
    "            else:\n",
    "                text_blob = text_blob.translate(from_lang=language, to=to_lang)\n",
    "                text_blob = text_blob.translate(from_lang=to_lang, to=language)\n",
    "                TRANSLATIONS[(message, to_lang)] = str(text_blob)\n",
    "        except NotTranslated:\n",
    "            pass\n",
    "        else:\n",
    "            augmented_messages.append(TRANSLATIONS[(message, to_lang)])\n",
    "\n",
    "    return augmented_messages\n",
    "\n",
    "\n",
    "def apply_balancing(data: pd.DataFrame, target: str, augmented: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply balancing to the dataset based on the selected strategy\n",
    "    \n",
    "    :param data: dataset to be balanced\n",
    "    :param target: target strategy (avg or max)\n",
    "    :param augmented: flag if we should apply augmentation (back-and-forth translation)\n",
    "    :return: balanced dataframe\n",
    "    \"\"\"\n",
    "    assert (augmented and target == \"avg\") or not augmented\n",
    "    \n",
    "    # get the file name with the augmented data\n",
    "    if augmented:\n",
    "        file_name = f\"balanced_augmented_{target}.parquet\"\n",
    "    else:\n",
    "        file_name = f\"balanced_{target}.parquet\"\n",
    "    \n",
    "    # if the file exists load and returns it\n",
    "    if os.path.exists(f\"../data/{file_name}\"):\n",
    "        return pd.read_parquet(f\"../data/{file_name}\")\n",
    "    \n",
    "    # select the training data\n",
    "    train = data.loc[lambda f: f[\"set\"] == \"train\"].copy()\n",
    "    train[\"augmented\"] = False\n",
    "    train[\"for_balance\"] = False\n",
    "    \n",
    "    # calculate the representation of each class\n",
    "    class_representation = train.goemotion.value_counts()\n",
    "    minority = class_representation.min()\n",
    "    majority = class_representation.max()\n",
    "    avg = int(class_representation.mean())\n",
    "\n",
    "    # choose what is the target amount of each class based on the strategy\n",
    "    if target == \"max\":\n",
    "        target_value = majority\n",
    "    elif target == \"avg\":\n",
    "        target_value = avg\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    # for each emotion of interest\n",
    "    final = list()\n",
    "    for emotion in class_representation.index:\n",
    "        # get the data with that emotion\n",
    "        edata = train.loc[lambda f: f[\"goemotion\"] == emotion]\n",
    "        representation = class_representation.loc[emotion]\n",
    "        to_generate = target_value - representation\n",
    "        \n",
    "        # if we have less than the target value, we should balance it\n",
    "        if representation < target_value:\n",
    "            # if we don't want any augmentation, do a simple sampling of the data\n",
    "            if not augmented:\n",
    "                sampled = pd.concat([edata, edata.sample(to_generate, replace=True).assign(for_balance=True)])\n",
    "            \n",
    "            # otherwise\n",
    "            else:\n",
    "                # sample some text from the original dataset\n",
    "                generated = list()\n",
    "                sampled = edata.sample(to_generate, replace=(to_generate > representation)).reset_index(drop=True)\n",
    "                \n",
    "                # for each text in the sample\n",
    "                for row in tqdm(sampled.itertuples(name=None), total=sampled.shape[0]):\n",
    "                    # try to perform the translation of text at least 3 times\n",
    "                    for i in range(3):\n",
    "                        try:\n",
    "                            sampled.loc[row[0], \"text\"] = data_augmentation(row[-4])[0]\n",
    "                            sampled.loc[row[0], \"augmented\"] = True\n",
    "                        except IndexError:\n",
    "                            continue\n",
    "                        except urllib.error.URLError:\n",
    "                            time.sleep(3)\n",
    "                        else:\n",
    "                            break\n",
    "                \n",
    "                # save the final dataset\n",
    "                sampled[\"for_balance\"] = True\n",
    "                sampled = pd.concat([edata, sampled])\n",
    "        else:\n",
    "            sampled = edata\n",
    "        \n",
    "        # save the sampled results to the final dataset\n",
    "        final.append(sampled)\n",
    "\n",
    "    # concatenate with the original test set\n",
    "    balanced = pd.concat(final + [data.loc[lambda f: f[\"set\"] != \"train\"].assign(augmented=False, for_balance=False)])\n",
    "    balanced = (\n",
    "        balanced.sort_values(by=[\"set\", \"code\", \"goemotion\"])\n",
    "        .assign(count=lambda f: f.groupby([\"code\", \"goemotion\"])[\"for_balance\"].cumsum().values)\n",
    "        .assign(code=lambda f: np.where(f[\"count\"] > 0, f[\"code\"] + \"_\" + f[\"count\"].astype(str), f[\"code\"]))\n",
    "        .assign(text=lambda f: f[\"text\"].apply(lambda x: x.replace(\"[name]\", \"[NAME]\")))\n",
    "        .assign(text=lambda f: f[\"text\"].apply(lambda x: x.replace(\"name]\", \"[NAME]\")))\n",
    "        .assign(text=lambda f: f[\"text\"].apply(lambda x: x.replace(\"[name\", \"[NAME]\")))\n",
    "        .assign(text=lambda f: f[\"text\"].apply(lambda x: x.replace(\"[religion]\", \"[RELIGION]\")))\n",
    "        .drop(columns=\"count\")\n",
    "    )\n",
    "    \n",
    "    # export the final result\n",
    "    balanced.to_parquet(f\"../data/{file_name}\")\n",
    "    \n",
    "    \n",
    "    # return the sampled dataset\n",
    "    return balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a80497",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:51.428854Z",
     "start_time": "2023-07-31T19:58:51.334887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>goemotion</th>\n",
       "      <th>ekman</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>set</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>augmented</th>\n",
       "      <th>for_balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ed8wbdn</td>\n",
       "      <td>admiration</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>Damn youtube and outrage drama is super lucrat...</td>\n",
       "      <td>damn youtube outrage drama super lucrative reddit</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ednhqdr</td>\n",
       "      <td>admiration</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>Famous for his 3-4 Defense</td>\n",
       "      <td>famous 3-4 defense</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eecj1hh</td>\n",
       "      <td>admiration</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>aw, thanks! I appreciate that!</td>\n",
       "      <td>aw thanks appreciate that</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eecj1hh</td>\n",
       "      <td>gratitude</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>aw, thanks! I appreciate that!</td>\n",
       "      <td>aw thanks appreciate that</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ed7nnui</td>\n",
       "      <td>admiration</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>LOL. Super cute!</td>\n",
       "      <td>lol super cute</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82426</th>\n",
       "      <td>efhc2in_1</td>\n",
       "      <td>desire</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>Poor [NAME]. I like him. I wish they would sho...</td>\n",
       "      <td>poor [NAME] like her wish would show going her</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82427</th>\n",
       "      <td>efhc2in_2</td>\n",
       "      <td>desire</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>Poor [NAME]. She liked me. I want to show what...</td>\n",
       "      <td>poor [NAME] like her wish would show going her</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82428</th>\n",
       "      <td>efhc2in_3</td>\n",
       "      <td>desire</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>Poor [NAME]. I like it. I want them to show wh...</td>\n",
       "      <td>poor [NAME] like her wish would show going her</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82429</th>\n",
       "      <td>efhc2in_4</td>\n",
       "      <td>desire</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>Bad [NAME]. I like her. I wish they could show...</td>\n",
       "      <td>poor [NAME] like her wish would show going her</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82430</th>\n",
       "      <td>efhc2in_5</td>\n",
       "      <td>desire</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>train</td>\n",
       "      <td>Bad [NAME]. I like her. I wish they would show...</td>\n",
       "      <td>poor [NAME] like her wish would show going her</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82431 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            code   goemotion ekman sentiment    set  \\\n",
       "0        ed8wbdn  admiration   joy  positive  train   \n",
       "1        ednhqdr  admiration   joy  positive  train   \n",
       "2        eecj1hh  admiration   joy  positive  train   \n",
       "3        eecj1hh   gratitude   joy  positive  train   \n",
       "4        ed7nnui  admiration   joy  positive  train   \n",
       "...          ...         ...   ...       ...    ...   \n",
       "82426  efhc2in_1      desire   joy  positive  train   \n",
       "82427  efhc2in_2      desire   joy  positive  train   \n",
       "82428  efhc2in_3      desire   joy  positive  train   \n",
       "82429  efhc2in_4      desire   joy  positive  train   \n",
       "82430  efhc2in_5      desire   joy  positive  train   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Damn youtube and outrage drama is super lucrat...   \n",
       "1                             Famous for his 3-4 Defense   \n",
       "2                        aw, thanks! I appreciate that!    \n",
       "3                        aw, thanks! I appreciate that!    \n",
       "4                                       LOL. Super cute!   \n",
       "...                                                  ...   \n",
       "82426  Poor [NAME]. I like him. I wish they would sho...   \n",
       "82427  Poor [NAME]. She liked me. I want to show what...   \n",
       "82428  Poor [NAME]. I like it. I want them to show wh...   \n",
       "82429  Bad [NAME]. I like her. I wish they could show...   \n",
       "82430  Bad [NAME]. I like her. I wish they would show...   \n",
       "\n",
       "                                              clean_text  augmented  \\\n",
       "0      damn youtube outrage drama super lucrative reddit      False   \n",
       "1                                     famous 3-4 defense      False   \n",
       "2                              aw thanks appreciate that      False   \n",
       "3                              aw thanks appreciate that      False   \n",
       "4                                         lol super cute      False   \n",
       "...                                                  ...        ...   \n",
       "82426     poor [NAME] like her wish would show going her       True   \n",
       "82427     poor [NAME] like her wish would show going her       True   \n",
       "82428     poor [NAME] like her wish would show going her       True   \n",
       "82429     poor [NAME] like her wish would show going her       True   \n",
       "82430     poor [NAME] like her wish would show going her       True   \n",
       "\n",
       "       for_balance  \n",
       "0            False  \n",
       "1            False  \n",
       "2            False  \n",
       "3            False  \n",
       "4            False  \n",
       "...            ...  \n",
       "82426         True  \n",
       "82427         True  \n",
       "82428         True  \n",
       "82429         True  \n",
       "82430         True  \n",
       "\n",
       "[82431 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_balancing(dataset, \"avg\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7326d",
   "metadata": {},
   "source": [
    "### Clean Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7209145a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:51.476030Z",
     "start_time": "2023-07-31T19:58:51.429854Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import contractions\n",
    "import emoji\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "TOKENIZER = TweetTokenizer()\n",
    "PUNCUATION_LIST = list(string.punctuation)\n",
    "\n",
    "\n",
    "def reinsert_tags(text: str, tag_split: str, tag_prefix: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Given a certain text, look for a separator that has been used for tag splitting\n",
    "    and make sure that all tags are put between []\n",
    "    \n",
    "    :param text: text to be adjusted\n",
    "    :param tag_split: tag to be splitted\n",
    "    :param tag_prefix: prefix to apply on tagging\n",
    "    :return: tagged text\n",
    "    \"\"\"\n",
    "    open_tag = False\n",
    "    full_sentence = \"\"\n",
    "    for sentence in text.split(tag_split):\n",
    "        if open_tag:\n",
    "            full_sentence += f\" [{tag_prefix}{sentence.upper().strip()}] \"\n",
    "        else:\n",
    "            full_sentence += sentence\n",
    "        open_tag = not open_tag\n",
    "\n",
    "    full_sentence = re.sub(\" +\", \" \", full_sentence)\n",
    "    return full_sentence.strip()\n",
    "\n",
    "\n",
    "def clean_content(\n",
    "    text, \n",
    "    fix_contraction: bool = True, \n",
    "    tag_emoji: bool = False,\n",
    "    tagged_items: list = [\"name\", \"religion\"],\n",
    "    handles: bool = True, \n",
    "    case: bool = False,\n",
    "    links: bool = True,\n",
    "    non_char: bool = False,\n",
    "    rm_stop_words: bool = False,\n",
    "    lemmatization: bool = True,\n",
    "    tokenize: bool = True,\n",
    "    ponctuation: bool = False,\n",
    "    unmapped_emoji: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Apply data cleaning to text given a range of options\n",
    "    \n",
    "    :param fix_contraction: True if we want to remove abbreviations\n",
    "    :param tag_emoji: True if we want to tag emojis by name\n",
    "    :param tagged_items: List of items that are tagged in the current text\n",
    "    :param handles: True if we want to remove twitter like handles\n",
    "    :param case: True if we want to normalize to lower case\n",
    "    :param links: True if we want to remove websites and links\n",
    "    :param non_char: True if we want to remove non-character words\n",
    "    :param rm_stop_words: True if we want to remove stop words\n",
    "    :param lemmatization: True if we want to apply lemmatization\n",
    "    :param tokenize: True if we want to apply twitter tokenization\n",
    "    :param ponctuation: True if we want to remove ponctuation\n",
    "    :param unmapped_emoji: True if we want to remove unmapped emojis\n",
    "    :return: clean text\n",
    "    \"\"\"\n",
    "    clean_text = text\n",
    "    \n",
    "    # replaces abbreviations with full word versions\n",
    "    if fix_contraction:\n",
    "        clean_text = contractions.fix(text)\n",
    "    \n",
    "    # replaces emojis\n",
    "    if tag_emoji:\n",
    "        clean_text = \"\".join(\n",
    "            [c if c not in emoji.EMOJI_DATA else emoji.EMOJI_DATA[c][\"en\"].replace(\":\", \" _emoji_ \") for c in clean_text]\n",
    "        )\n",
    "    \n",
    "    # remove reddit handles\n",
    "    if handles:\n",
    "        clean_text = re.sub(r\"@\\w+\\s?\", \"\", clean_text)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    if case:\n",
    "        clean_text = clean_text.lower()\n",
    "    \n",
    "\n",
    "    if links:\n",
    "         # remove links http:// or https://\n",
    "        clean_text = re.sub(r\"https?:\\/\\/\\S+\", \"\", clean_text)\n",
    "    \n",
    "        # remove links beginning with www. and ending with .com\n",
    "        clean_text = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", \"\", clean_text)\n",
    "    \n",
    "        # remove html reference characters\n",
    "        clean_text = re.sub(r\"&[a-z]+;\", \"\", clean_text)\n",
    "        \n",
    "    # deal with tagged items\n",
    "    for tag in tagged_items:\n",
    "        clean_text = re.sub(fr\"\\[{tag}\\]\", f\" _tag_ {tag} _tag_ \", clean_text)  \n",
    "    \n",
    "    # remove non-letter characters besides spaces \"/\", \";\" \"[\", \"]\" \"=\", \"#\"\n",
    "    if non_char:\n",
    "        clean_text = re.sub(r\"[/;\\[\\]=#]\", \"\", clean_text)  \n",
    "    \n",
    "    # remove stop words\n",
    "    if rm_stop_words:\n",
    "        clean_lst = []\n",
    "        for word in clean_text.split():\n",
    "            if word not in STOP_WORDS:\n",
    "                clean_lst.append(word)\n",
    "    else:\n",
    "        clean_lst = clean_text.split()\n",
    "        \n",
    "    \n",
    "    # apply lemmatization\n",
    "    if lemmatization:\n",
    "        lemmatized_words = []\n",
    "        for word in clean_lst:\n",
    "            lemmatized_word = WordNetLemmatizer().lemmatize(word)\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "        clean_lst = lemmatized_words\n",
    "    \n",
    "    # concatenate the text again\n",
    "    clean_text = \" \".join(clean_lst)\n",
    "    \n",
    "    # apply tokenization\n",
    "    if tokenize:\n",
    "        tokens = TOKENIZER.tokenize(clean_text)\n",
    "    else:\n",
    "        tokens = clean_text.split(\" \")\n",
    "    \n",
    "    if ponctuation:\n",
    "        clean_text = \" \".join([w for w in tokens if w not in PUNCUATION_LIST])\n",
    "    else:\n",
    "        clean_text = \" \".join(tokens)\n",
    "        \n",
    "    # clean emojis that were not mapped by the library\n",
    "    if unmapped_emoji:\n",
    "        clean_text = \"\".join([w for w in clean_text if ord(w) < 2000])\n",
    "        \n",
    "    # add the tags for emojis\n",
    "    if tag_emoji:\n",
    "        clean_text = reinsert_tags(clean_text, \"_emoji_\", \"EMOJI_\")\n",
    "    \n",
    "    # re-insert tags\n",
    "    if len(tagged_items) > 0:\n",
    "        clean_text = reinsert_tags(clean_text, \"_tag_\")\n",
    "        \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3a86fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.486019Z",
     "start_time": "2023-07-31T19:58:51.479019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It doesn't, it just resets back to 1 *before* adding in your upvote. You're not gaming the system lol\n",
      "It doe not , it just reset back to 1 * before * adding in your upvote . You are not gaming the system lol\n"
     ]
    }
   ],
   "source": [
    "text = dataset.text.sample(1).values[0]\n",
    "print(text)\n",
    "print(clean_content(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1515073d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.502025Z",
     "start_time": "2023-07-31T19:58:52.488019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No . Or at least that is what I suspect ðŸ¤”'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_content(\"No. Or at least thatâ€™s what I suspectðŸ¤”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b05c3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.517018Z",
     "start_time": "2023-07-31T19:58:52.503019Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No. Or at least thats what I suspect [EMOJI_THINKING_FACE]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_content(\n",
    "    \"No. Or at least thatâ€™s what I suspectðŸ¤”\", \n",
    "    fix_contraction=False, \n",
    "    tag_emoji=True,\n",
    "    tagged_items=[\"NAME\", \"RELIGION\"],\n",
    "    handles=False, \n",
    "    case=False,\n",
    "    links=False,\n",
    "    non_char=False,\n",
    "    rm_stop_words=False,\n",
    "    lemmatization=False,\n",
    "    tokenize=False,\n",
    "    ponctuation=False,\n",
    "    unmapped_emoji=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ea0056",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8608bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.757051Z",
     "start_time": "2023-07-31T19:58:52.518018Z"
    }
   },
   "outputs": [],
   "source": [
    "EMOJIS_FOUND = (\n",
    "    pd.DataFrame(\n",
    "        [\n",
    "            (c,) \n",
    "            for text in dataset[\"text\"].to_list()\n",
    "            for c in text\n",
    "            if c in emoji.EMOJI_DATA \n",
    "        ],\n",
    "        columns=[\"emoji\"]\n",
    "    )\n",
    "    .assign(count=1)\n",
    "    .groupby([\"emoji\"], as_index=False)[\"count\"]\n",
    "    .sum()\n",
    "    .assign(pct=lambda f: f[\"count\"] / f[\"count\"].sum())\n",
    "    .assign(cum_pct=lambda f: f[\"pct\"].cumsum())\n",
    ")\n",
    "\n",
    "\n",
    "def apply_tokenization(\n",
    "    t_model: TFAutoModel, \n",
    "    tokenizer: AutoTokenizer, \n",
    "    data: pd.DataFrame, \n",
    "    emoji_tagging: bool = False, \n",
    "    emoji_threshold: float = 0.8\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Given a tokenizer object and some data, apply the tokenization process on the\n",
    "    train, test and validation sets and generate a dictionary of processed data with\n",
    "    all inputs needed for the model\n",
    "    \n",
    "    :param t_model: transformer model\n",
    "    :param tokenizer: tokenizer object\n",
    "    :param data: input data to be used for training\n",
    "    :param emoji_tagging: flag if we should apply emoji tagging\n",
    "    :param emoji_threshold: threshold for selection of emojis to be added\n",
    "    \"\"\"\n",
    "    processed = dict()\n",
    "    \n",
    "    # select words to be added\n",
    "    words_to_add = [\"[NAME]\", \"[RELIGION]\"]\n",
    "    if emoji_tagging:\n",
    "        words_to_add += EMOJIS_FOUND.loc[lambda f: f[\"cum_pct\"] <= emoji_threshold].emoji.to_list()\n",
    "        \n",
    "    # add the new tokens\n",
    "    tokenizer.add_tokens(words_to_add)\n",
    "    t_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # for each group of data\n",
    "    for group in tqdm([\"train\", \"validation\", \"test\"]):\n",
    "        # pivot the adtaset and extract the emotions\n",
    "        df = data.loc[lambda f: f[\"set\"] == group].pivot_table(\n",
    "            index=[\"code\", \"text\"],\n",
    "            columns=[\"goemotion\"],\n",
    "            values=\"set\",\n",
    "            aggfunc=\"count\",\n",
    "            fill_value=0\n",
    "        ).reset_index().drop(columns=[\"none\"], errors=\"ignore\")\n",
    "\n",
    "        processed[group] = dict()\n",
    "        \n",
    "        # apply the tokenizer to the data\n",
    "        processed[group][\"tokens\"] = tokenizer(\n",
    "            df.text.to_list(), \n",
    "            max_length=MAX_SEQUENCE_LENGTH, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_tensors=\"tf\"\n",
    "        )\n",
    "        \n",
    "        # create same inputs to be used in the model\n",
    "        processed[group][\"inputs\"] = [\n",
    "            processed[group][\"tokens\"].input_ids, \n",
    "            processed[group][\"tokens\"].token_type_ids, \n",
    "            processed[group][\"tokens\"].attention_mask\n",
    "        ]\n",
    "        \n",
    "        # create the set of labels\n",
    "        processed[group][\"labels\"] = df.iloc[:, 2:].values\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4763186",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "939daff3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.773018Z",
     "start_time": "2023-07-31T19:58:52.758018Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_cls_model(\n",
    "    t_model: TFAutoModel,\n",
    "    trainable: str = \"all\",\n",
    "    head: str = \"none\",\n",
    "    dropout: float = 0.3,\n",
    "    label_smoothing: float = 0.1,\n",
    "    hidden_size: int = 256, \n",
    "    hidden_layers: list = [256, 128, 64],\n",
    "    dropout_layers: list = [0.3, 0.3, 0.3],\n",
    "    num_filters: list = [100, 100, 50, 25],\n",
    "    kernel_sizes: list = [2, 3, 4, 5],\n",
    "    learning_rate: float = 0.00005,\n",
    "    epsilon: float = 1e-08,\n",
    "    num_classes: int = 28,\n",
    "    max_sequence_length: int = MAX_SEQUENCE_LENGTH,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a classification model using a pre-trained transformer\n",
    "    \n",
    "    :param t_model: transformer model\n",
    "    :param trainable: select whith parts of the transformer model are trainable (all, last, none)\n",
    "    :param head: type of head to be applied (none, dense, mlp, cnn)\n",
    "    :param dropout: dropout value to be selected\n",
    "    :param label_smoothing: label smoothing to be applied\n",
    "    :param hidden_size: number of nodes for head=dense\n",
    "    :param hidden_layers: number of nodes per layer for head=mlp\n",
    "    :param dropout_layers: dropout rate for each hidden layer for head=mlp\n",
    "    :param num_filters: number of filters to use for head=cnn\n",
    "    :param kernel_sizes: kernal sizes to use for head=cnn\n",
    "    :param learning_rate: learning rate applied for Adam\n",
    "    :param epsilon: epsilon selected for Adam\n",
    "    :param num_classes: number of classes to predict\n",
    "    :param max_sequence_length: maximum sequence length selected\n",
    "    \"\"\"\n",
    "    # set the model to be trainable\n",
    "    if trainable == \"all\":\n",
    "        t_model.trainable = True\n",
    "    elif trainable == \"last\":\n",
    "        last_layer_num = max(\n",
    "            [\n",
    "                int(w.name[w.name.index(\"layer_._\"):].split(\"/\")[0].replace(\"layer_._\", \"\"))\n",
    "                for w in t_model.weights if \"layer_._\" in w.name\n",
    "            ]\n",
    "        )\n",
    "        for w in t_model.weights:\n",
    "            if f\"layer_._{last_layer_num}\" not in w.name:\n",
    "                w._trainable = False\n",
    "    elif trainable == \"none\":\n",
    "        t_model.trainable = False\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # extract input ids, token ids and the attention mask\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name=\"input_ids_layer\")\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name=\"token_type_ids_layer\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int64, name=\"attention_mask_layer\")\n",
    "    \n",
    "    # encode this into the model\n",
    "    model_inputs = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}      \n",
    "    output = t_model(model_inputs)\n",
    "    \n",
    "    # if no head was selected, pass the pooler token to a dropout layer\n",
    "    if head == \"none\":\n",
    "        pooler_token = output[1]\n",
    "        hidden = tf.keras.layers.Dropout(dropout)(pooler_token)\n",
    "        \n",
    "    # if a dense head was selected, add a hidden layer with the selected hidden size\n",
    "    elif head == \"dense\":\n",
    "        pooler_token = output[1]\n",
    "        hidden = tf.keras.layers.Dense(hidden_size, activation=\"relu\", name=\"hidden_layer\")(pooler_token)\n",
    "        hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
    "    \n",
    "    # if multi-layer perceptron was selected, add the hidden layers on top of the pooler token\n",
    "    elif head == \"mlp\":\n",
    "        pooler_token = output[1]\n",
    "        hidden = tf.keras.layers.Dense(hidden_layers[0], activation=\"relu\", name=\"hidden_layer\")(pooler_token)\n",
    "        i = 0\n",
    "        for size in hidden_layers[1:]:\n",
    "            hidden = tf.keras.layers.Dropout(dropout_layers[i])(hidden)\n",
    "            hidden = tf.keras.layers.Dense(size, activation=\"relu\", name=\"hidden_layer\")(hidden)\n",
    "            i += 1\n",
    "        hidden = tf.keras.layers.Dropout(dropout_layers[-1])(hidden)\n",
    "    \n",
    "    # if cnn was selected, get the token embeddings and create a cnn network\n",
    "    elif head == \"cnn\":\n",
    "        token_embeddings = output[0]\n",
    "        cnn_outputs = []\n",
    "        for filters, kernel_size in zip(num_filters, kernel_sizes):\n",
    "            conv_layer = tf.keras.layers.Conv1D(\n",
    "                filters=filters, kernel_size=kernel_size, activation='relu'\n",
    "            )(token_embeddings)\n",
    "            max_pool = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "            cnn_outputs.append(max_pool)\n",
    "        cnn_concat = tf.keras.layers.concatenate(cnn_outputs)\n",
    "        \n",
    "        hidden = tf.keras.layers.Dense(hidden_size, activation='relu', name='hidden_layer')(cnn_concat)\n",
    "        hidden = tf.keras.layers.Dropout(dropout)(hidden)\n",
    "    \n",
    "    # with the final hidden layer, add a dense layer for the classification task\n",
    "    classification = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"classification_layer\")(hidden)\n",
    "    \n",
    "    # instantiate the classification model\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[classification])\n",
    "    \n",
    "    # compile using the learning rate and selected epsilon\n",
    "    classification_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing), \n",
    "        metrics=[tfa.metrics.F1Score(num_classes=num_classes, average=\"macro\", threshold=0.2)]\n",
    "    )\n",
    "    \n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc372e38",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca6e9edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.788018Z",
     "start_time": "2023-07-31T19:58:52.774019Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "TARGET_NAMES = sorted(list(dataset[\"goemotion\"].unique()))\n",
    "\n",
    "\n",
    "def evaluate_model(p_set: str, proba: pd.DataFrame, dataset: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the model results\n",
    "    \n",
    "    :param p_set: which prediction set to use (test, validation)\n",
    "    :param dataset: the complete dataset\n",
    "    :return: model evaluation values\n",
    "    \"\"\"\n",
    "    outputs = dict()\n",
    "    df = dataset.loc[lambda f: f[\"set\"] == p_set]\n",
    "    \n",
    "    # get the one-hot-encoded values\n",
    "    pv = df.pivot_table(index=[\"code\"], columns=\"goemotion\", values=\"set\", aggfunc=\"count\", fill_value=0)\n",
    "    \n",
    "    # generate the predictions\n",
    "    proba = proba.loc[pv.index, pv.columns]\n",
    "    predictions = (proba.values > 0.2).astype(int)\n",
    "    pred = (\n",
    "        proba.reset_index()\n",
    "        .rename(columns={\"index\": \"code\"})\n",
    "        .melt(id_vars=[\"code\"], var_name=\"goemotion\", value_name=\"proba\")\n",
    "    )\n",
    "    pred[\"flag\"] = pred[\"proba\"] > 0.2\n",
    "    outputs[\"predictions\"] = pred\n",
    "    \n",
    "    # calculate metrics\n",
    "    outputs[\"f1_macro\"] = f1_score(pv[TARGET_NAMES].values, predictions, average=\"macro\")\n",
    "    outputs[\"f1_micro\"] = f1_score(pv[TARGET_NAMES].values, predictions, average=\"micro\")\n",
    "    outputs[\"roc_auc\"] = roc_auc_score(pv[TARGET_NAMES].values, proba.values, average=\"macro\", multi_class=\"ovo\")\n",
    "    outputs[\"confusion_matrix\"] = confusion_matrix(\n",
    "        np.argmax(pv[TARGET_NAMES].values, axis=1), np.argmax(predictions, axis=1)\n",
    "    )\n",
    "    outputs[\"classification_report\"] = classification_report(\n",
    "        pv[TARGET_NAMES].values, predictions, target_names=TARGET_NAMES\n",
    "    )\n",
    "    \n",
    "    # get the misclassification value\n",
    "    df = df.merge(pred.loc[lambda f: f[\"flag\"] == 1], on=[\"code\", \"goemotion\"], how=\"left\")\n",
    "    corrclass = df[df[\"flag\"].notnull()]\n",
    "    misclass = df[df[\"flag\"].isnull()]\n",
    "    outputs[\"misclassification\"] = misclass\n",
    "    \n",
    "    # get misclassification examples\n",
    "    outputs[\"misclassification_examples\"] = {\n",
    "        label: misclass[misclass[\"goemotion\"] == label]\n",
    "        .sample(3, replace=True)\n",
    "        .drop_duplicates()\n",
    "        .text\n",
    "        .to_list()\n",
    "        for label in TARGET_NAMES\n",
    "        if misclass[misclass[\"goemotion\"] == label].shape[0] > 0\n",
    "    }\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5f02af",
   "metadata": {},
   "source": [
    "### Scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9d50cfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.803017Z",
     "start_time": "2023-07-31T19:58:52.789019Z"
    }
   },
   "outputs": [],
   "source": [
    "def scheduler_10(epoch, lr):\n",
    "    return lr\n",
    "\n",
    "\n",
    "def scheduler_05(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "    \n",
    "def scheduler_02(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * 0.2\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "    \n",
    "def scheduler_exp(epoch, lr):\n",
    "    if epoch > 0:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638956db",
   "metadata": {},
   "source": [
    "### Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c214d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.851017Z",
     "start_time": "2023-07-31T19:58:52.804018Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import ipynbname\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "NB_FNAME = ipynbname.name()\n",
    "\n",
    "SCHEDULERS = {\"1.0\": scheduler_10, \"0.5\": scheduler_05, \"0.2\": scheduler_02, \"exp\": scheduler_exp}\n",
    "\n",
    "\n",
    "def limit_mem():\n",
    "    tf.compat.v1.keras.backend.get_session().close()\n",
    "    cfg = tf.compat.v1.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=cfg))\n",
    "    \n",
    "\n",
    "\n",
    "def run_model_experiment(grid: dict, dataset: pd.DataFrame):   \n",
    "    # select the appropriate dataset based on the balancing parameter\n",
    "    if grid[\"balancing\"] != \"none\":\n",
    "        if isinstance(grid[\"balancing\"], list):\n",
    "            data_for_exp = apply_balancing(dataset, \"avg\", False)\n",
    "            data_for_exp = data_for_exp.loc[\n",
    "                lambda f: (f[\"set\"] != \"train\") | (\n",
    "                    (f[\"set\"] == \"train\") & (~f[\"goemotion\"].isin(grid[\"balancing\"])) & (~f[\"for_balance\"])\n",
    "                ) | (\n",
    "                    (f[\"set\"] == \"train\") & (f[\"goemotion\"].isin(grid[\"balancing\"]))\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            data_for_exp = apply_balancing(dataset, grid[\"balancing\"], grid[\"augment\"])\n",
    "    else:\n",
    "        data_for_exp = dataset.copy()\n",
    "\n",
    "    # select the combinations to execute\n",
    "    combinations = {k: (k,) for k in data_for_exp.goemotion.unique()}\n",
    "    if len(grid[\"minority_shuffling\"]) > 0:\n",
    "        combinations = grid[\"minority_shuffling\"]\n",
    "\n",
    "    # apply cleaning to the dataset\n",
    "    if grid[\"clean_data\"]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # get the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(grid[\"model\"])\n",
    "    try:\n",
    "        t_model = TFAutoModel.from_pretrained(grid[\"model\"])\n",
    "    except OSError:\n",
    "        t_model = TFAutoModel.from_pretrained(grid[\"model\"], from_pt=True)\n",
    "\n",
    "    # get how many model replacements we should make\n",
    "    replaces = [{v: k for k, vl in combinations.items() for v in vl}]\n",
    "    for k, vl in combinations.items():\n",
    "        if len(vl) > 1:\n",
    "            replaces.append({v: v for v in vl})\n",
    "            \n",
    "    # create the folder\n",
    "    folder = Path(\"../experiments/\" + pd.to_datetime(\"today\").strftime(\"%Y%m%dT%H%M%S\"))\n",
    "    folder.mkdir(exist_ok=False, parents=True)\n",
    "\n",
    "    # run the model loop\n",
    "    predictions = dict()\n",
    "    for i, comb in enumerate(replaces):\n",
    "        # select the data to be used for training\n",
    "        data_for_training = (\n",
    "            data_for_exp.loc[lambda f: ((f[\"goemotion\"].isin(comb)) & (f[\"set\"] == \"train\")) | (f[\"set\"] != \"train\")]\n",
    "            .assign(goemotion=lambda f: f[\"goemotion\"].apply(lambda x: \"none\" if x not in comb else x))\n",
    "            .assign(goemotion=lambda f: f[\"goemotion\"].replace(comb))\n",
    "        )\n",
    "\n",
    "        # apply tokenization\n",
    "        processed = apply_tokenization(t_model, tokenizer, data_for_training, grid[\"emoji_tagging\"])\n",
    "\n",
    "        # create the model\n",
    "        cls_model = create_cls_model(\n",
    "            t_model, \n",
    "            trainable=grid[\"trainable\"], \n",
    "            head=grid[\"head\"], \n",
    "            dropout=grid.get(\"dropout\", 0.3),\n",
    "            label_smoothing=grid[\"label_smoothing\"],\n",
    "            learning_rate=grid[\"learning_rate\"],\n",
    "            num_classes=data_for_training.loc[lambda f: f[\"set\"] == \"train\"].goemotion.nunique(),\n",
    "        )\n",
    "        \n",
    "        # get the indexes that make the validation set\n",
    "        indexes = (\n",
    "            data_for_training.loc[lambda f: f[\"set\"] == \"validation\"]\n",
    "            .pivot_table(index=\"code\", columns=[\"goemotion\"], values=\"set\", aggfunc=\"count\", fill_value=0)\n",
    "            .drop(columns=[\"none\"], errors=\"ignore\")\n",
    "            .sum(axis=1)\n",
    "            .reset_index()\n",
    "            .loc[lambda f: f[0] > 0]\n",
    "            .index\n",
    "        )\n",
    "        val_inputs = [tf.convert_to_tensor(tensor.numpy()[indexes]) for tensor in processed[\"validation\"][\"inputs\"]]\n",
    "        val_labels = processed[\"validation\"][\"labels\"][indexes]\n",
    "\n",
    "        # fit the model\n",
    "        cb_scheduler = tf.keras.callbacks.LearningRateScheduler(SCHEDULERS[grid[\"scheduler\"]])\n",
    "        cb_earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model_history = cls_model.fit(\n",
    "            processed[\"train\"][\"inputs\"],\n",
    "            processed[\"train\"][\"labels\"],\n",
    "            validation_data=(val_inputs, val_labels),\n",
    "            batch_size=grid[\"batch_size\"],\n",
    "            epochs=grid[\"epochs\"],\n",
    "            callbacks=[cb_scheduler, cb_earlystop],\n",
    "        )\n",
    "        \n",
    "        # save the history data\n",
    "        with open(folder / f\"history_{i}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(model_history.history, f)\n",
    "\n",
    "        # run the probability calculation\n",
    "        classes = sorted(list(set(comb.values())))\n",
    "        predictions[tuple(classes)] = dict()\n",
    "        for p_set in [\"validation\", \"test\"]:\n",
    "            index = (\n",
    "                data_for_training.loc[lambda f: f[\"set\"] == p_set]\n",
    "                .pivot_table(index=\"code\", columns=[\"goemotion\"], values=\"set\", aggfunc=\"count\", fill_value=0)\n",
    "                .index\n",
    "            )\n",
    "            predictions[tuple(classes)][p_set] = pd.DataFrame(\n",
    "                cls_model.predict(processed[p_set][\"inputs\"]), index=index, columns=classes,\n",
    "            )\n",
    "    \n",
    "    # export the base predictions\n",
    "    with open(folder / \"original_predictions.pkl\", \"wb\") as f:\n",
    "        pickle.dump(predictions, f)\n",
    "    \n",
    "    # ensure for the post predictions that we have the expected values\n",
    "    base = predictions[list(predictions)[0]]\n",
    "    for k, vl in combinations.items():\n",
    "        if len(vl) > 1:\n",
    "            values = predictions[tuple(sorted(vl))]\n",
    "            for p_set in [\"validation\", \"test\"]:\n",
    "                base[p_set] = pd.concat(\n",
    "                    [base[p_set].drop(columns=k), values[p_set].multiply(base[p_set][k], axis=0)], axis=1\n",
    "                )\n",
    "\n",
    "    # run the model evaluation on validation\n",
    "    val_res = evaluate_model(\"validation\", base[\"validation\"], data_for_exp)\n",
    "    test_res = evaluate_model(\"test\", base[\"test\"], data_for_exp)\n",
    "    \n",
    "    # save the model val_res\n",
    "    with open(folder / \"grid_params.json\", \"w\") as f:\n",
    "        json.dump(grid, f)\n",
    "    \n",
    "    for res, name in [(val_res, \"validation\"), (test_res, \"test\")]:\n",
    "        with open(folder / f\"metrics_{name}.json\", \"w\") as f:\n",
    "            json.dump(\n",
    "                {\n",
    "                    r: v \n",
    "                    for r, v in res.items() \n",
    "                    if r not in [\"confusion_matrix\", \"predictions\", \"misclassification\"]\n",
    "                }, \n",
    "                f\n",
    "            )\n",
    "        with open(folder / f\"confusion_matrix_{name}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(res[\"confusion_matrix\"], f)\n",
    "\n",
    "        res[\"predictions\"].to_pickle(folder / f\"prediction_{name}.pkl\")\n",
    "        res[\"misclassification\"].to_pickle(folder / f\"misclassification_{name}.pkl\")\n",
    "    \n",
    "    data_for_exp.to_pickle(folder / \"data_for_exp.pkl\")\n",
    "    \n",
    "    shutil.copyfile(os.path.abspath(f\"{NB_FNAME}.ipynb\"), folder / f\"{NB_FNAME}.ipynb\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca04700b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-30T00:34:44.000330Z",
     "start_time": "2023-07-30T00:32:58.291018Z"
    }
   },
   "source": [
    "np.random.seed(33)\n",
    "tf.random.set_seed(33)\n",
    "\n",
    "grid = {\n",
    "    \"model\": \"bert-base-uncased\",\n",
    "    \"trainable\": \"last\",\n",
    "    \"head\": \"none\",\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"scheduler\": \"1.0\",\n",
    "    \"dropout\": 0.3,\n",
    "    \"label_smoothing\": 0.05,\n",
    "    \"learning_rate\": 0.00005,\n",
    "    \"emoji_tagging\": True,\n",
    "    \"clean_data\": False,\n",
    "    \"balancing\": \"none\",\n",
    "    \"augment\": False,\n",
    "    \"minority_shuffling\": {\n",
    "        \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "        \"nervousness\": (\"nervousness\", ),\n",
    "        \"fear\": (\"fear\", ),\n",
    "        \"pride\": (\"pride\", ),\n",
    "        \"approval\": (\"approval\", ),\n",
    "        \"realization\": (\"realization\", ),\n",
    "        \"surprise\": (\"surprise\", ),\n",
    "        \"relief\": (\"relief\",),\n",
    "        \"joy\": (\"joy\",),\n",
    "        \"neutral\": (\"neutral\",),\n",
    "        \"optimism\": (\"optimism\",),\n",
    "        \"anger\": (\"anger\",),\n",
    "        \"desire\": (\"desire\",),\n",
    "        \"love\": (\"love\",),\n",
    "        \"disapproval\": (\"disapproval\",),\n",
    "        \"amusement\": (\"amusement\",),\n",
    "        \"caring\": (\"caring\",),\n",
    "        \"excitement\": (\"excitement\",),\n",
    "        \"curiosity\": (\"curiosity\",),\n",
    "        \"embarrassment\": (\"embarrassment\",),\n",
    "        \"disgust\": (\"disgust\",),\n",
    "        \"gratitude\": (\"gratitude\",),\n",
    "        \"annoyance\": (\"annoyance\",),\n",
    "        \"confusion\": (\"confusion\",),\n",
    "        \"disappointment\": (\"disappointment\",),\n",
    "        \"admiration\": (\"admiration\",),\n",
    "        \"remorse\": (\"remorse\",)\n",
    "    },\n",
    "}\n",
    "\n",
    "limit_mem()\n",
    "tf.keras.backend.clear_session()\n",
    "while gc.collect():\n",
    "    continue\n",
    "run_model_experiment(grid, dataset.sample(frac=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f321ad8b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225f6ad",
   "metadata": {},
   "source": [
    "## Grid Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "417f89fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.883017Z",
     "start_time": "2023-07-31T19:58:52.852018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160 searches created - expected time: 500.0 days\n"
     ]
    }
   ],
   "source": [
    "GRID = {\n",
    "    \"model\": [\n",
    "        \"bert-base-uncased\",\n",
    "        \"vinai/bertweet-base\",\n",
    "        \"flboehm/reddit-bert-text_10\",\n",
    "    ],\n",
    "    \"trainable\": [\"all\", \"last\", \"none\"],\n",
    "    \"head\": [\"none\", \"dense\", \"cnn\", \"mlp\"],\n",
    "    \"dropout\": [0.3],\n",
    "    \"label_smoothing\": [0, 0.1],\n",
    "    \"epochs\": [10],\n",
    "    \"batch_size\": [8, 16],\n",
    "    \"learning_rate\": [0.00005],\n",
    "    \"emoji_tagging\": [False, True],\n",
    "    \"clean_data\": [False, True],\n",
    "    \"balancing\": [\"none\", \"avg\"],\n",
    "    \"augment\": [False, True],\n",
    "    \"minority_shuffling\": [\n",
    "        dict(),\n",
    "        {\n",
    "            \"neutral\": [\"neutral\"],\n",
    "            \"anger\": [\"anger\", \"annoyance\", \"disapproval\"],\n",
    "            \"disgust\": [\"disgust\"],\n",
    "            \"fear\": [\"fear\", \"nervousness\"],\n",
    "            \"joy\": [\n",
    "                \"joy\", \n",
    "                \"amusement\",\n",
    "                \"approval\", \n",
    "                \"excitement\", \n",
    "                \"gratitude\",  \n",
    "                \"love\", \n",
    "                \"optimism\", \n",
    "                \"relief\", \n",
    "                \"pride\", \n",
    "                \"admiration\", \n",
    "                \"desire\", \n",
    "                \"caring\"\n",
    "            ],\n",
    "            \"sadness\": [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\",  \"remorse\"],\n",
    "            \"surprise\": [\"surprise\", \"realization\", \"confusion\", \"curiosity\"]\n",
    "        },\n",
    "        {\n",
    "            \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "            \"nervousness_fear\": (\"nervousness\", \"fear\"),\n",
    "            \"pride_approval\": (\"pride\", \"approval\"),\n",
    "            \"realization_surprise\": (\"realization\", \"surprise\"),\n",
    "            \"relief_joy\": (\"relief\", \"joy\"),\n",
    "            \"neutral\": (\"neutral\",),\n",
    "            \"optimism\": (\"optimism\",),\n",
    "            \"anger\": (\"anger\",),\n",
    "            \"desire\": (\"desire\",),\n",
    "            \"love\": (\"love\",),\n",
    "            \"disapproval\": (\"disapproval\",),\n",
    "            \"amusement\": (\"amusement\",),\n",
    "            \"caring\": (\"caring\",),\n",
    "            \"excitement\": (\"excitement\",),\n",
    "            \"curiosity\": (\"curiosity\",),\n",
    "            \"embarrassment\": (\"embarrassment\",),\n",
    "            \"disgust\": (\"disgust\",),\n",
    "            \"gratitude\": (\"gratitude\",),\n",
    "            \"annoyance\": (\"annoyance\",),\n",
    "            \"confusion\": (\"confusion\",),\n",
    "            \"disappointment\": (\"disappointment\",),\n",
    "            \"admiration\": (\"admiration\",),\n",
    "            \"remorse\": (\"remorse\",)\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "EARLY_STOP = 0.55\n",
    "\n",
    "keys, values = zip(*GRID.items())\n",
    "grid_space = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "valid_space = list()\n",
    "for g in grid_space:\n",
    "    if g[\"clean_data\"] and not g[\"emoji_tagging\"]:\n",
    "        continue\n",
    "    if g[\"augment\"] and g[\"balancing\"] == \"none\":\n",
    "        continue\n",
    "    if len(g[\"minority_shuffling\"]) > 0 and g[\"balancing\"] != \"none\":\n",
    "        continue\n",
    "    if len(g[\"minority_shuffling\"]) > 0 and g[\"augment\"]:\n",
    "        continue\n",
    "    valid_space.append(g)\n",
    "    \n",
    "print(len(valid_space), \"searches created - expected time:\", len(valid_space) * 10000 * 2 / (24 * 60 * 60), \"days\")\n",
    "for _ in range(10):\n",
    "    random.shuffle(valid_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0d3e2d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-31T19:58:52.898019Z",
     "start_time": "2023-07-31T19:58:52.884018Z"
    }
   },
   "outputs": [],
   "source": [
    "BOV = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"relief_amusement\": (\"relief\", \"amusement\"),\n",
    "    \"nervousness\": (\"nervousness\", ),\n",
    "    \"fear\": (\"fear\", ),\n",
    "    \"anger\": (\"anger\", ),\n",
    "    \"annoyance\": (\"annoyance\", ),\n",
    "    \"caring\": (\"caring\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"joy\": (\"joy\", ),\n",
    "    \"neutral\": (\"neutral\", ),\n",
    "    \"optimism\": (\"optimism\", ),\n",
    "    \"desire\": (\"desire\", ),\n",
    "    \"love\": (\"love\", ),\n",
    "    \"disapproval\": (\"disapproval\", ),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"excitement\": (\"excitement\", ),\n",
    "    \"curiosity\": (\"curiosity\", ),\n",
    "    \"embarrassment\": (\"embarrassment\", ),\n",
    "    \"disgust\": (\"disgust\", ),\n",
    "    \"gratitude\": (\"gratitude\", ),\n",
    "    \"confusion\": (\"confusion\", ),\n",
    "    \"disappointment\": (\"disappointment\", ),\n",
    "    \"remorse\": (\"remorse\", )\n",
    "}\n",
    "\n",
    "BOV_2 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"relief_amusement\": (\"relief\", \"amusement\"),\n",
    "    \"nervousness_fear\": (\"nervousness\", \"fear\"),\n",
    "    \"anger\": (\"anger\", ),\n",
    "    \"annoyance\": (\"annoyance\", ),\n",
    "    \"caring\": (\"caring\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"joy\": (\"joy\", ),\n",
    "    \"neutral\": (\"neutral\", ),\n",
    "    \"optimism\": (\"optimism\", ),\n",
    "    \"desire\": (\"desire\", ),\n",
    "    \"love\": (\"love\", ),\n",
    "    \"disapproval\": (\"disapproval\", ),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"excitement\": (\"excitement\", ),\n",
    "    \"curiosity\": (\"curiosity\", ),\n",
    "    \"embarrassment\": (\"embarrassment\", ),\n",
    "    \"disgust\": (\"disgust\", ),\n",
    "    \"gratitude\": (\"gratitude\", ),\n",
    "    \"confusion\": (\"confusion\", ),\n",
    "    \"disappointment\": (\"disappointment\", ),\n",
    "    \"remorse\": (\"remorse\", )\n",
    "}\n",
    "\n",
    "BOV_3 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"relief_amusement\": (\"relief\", \"amusement\"),\n",
    "    \"anger_annoyance\": (\"anger\", \"annoyance\"),\n",
    "    \"nervousness\": (\"nervousness\", ),\n",
    "    \"fear\": (\"fear\", ),\n",
    "    \"caring\": (\"caring\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"joy\": (\"joy\", ),\n",
    "    \"neutral\": (\"neutral\", ),\n",
    "    \"optimism\": (\"optimism\", ),\n",
    "    \"desire\": (\"desire\", ),\n",
    "    \"love\": (\"love\", ),\n",
    "    \"disapproval\": (\"disapproval\", ),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"excitement\": (\"excitement\", ),\n",
    "    \"curiosity\": (\"curiosity\", ),\n",
    "    \"embarrassment\": (\"embarrassment\", ),\n",
    "    \"disgust\": (\"disgust\", ),\n",
    "    \"gratitude\": (\"gratitude\", ),\n",
    "    \"confusion\": (\"confusion\", ),\n",
    "    \"disappointment\": (\"disappointment\", ),\n",
    "    \"remorse\": (\"remorse\", )\n",
    "}\n",
    "\n",
    "BOV_4 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"relief_amusement\": (\"relief\", \"amusement\"),\n",
    "    \"nervousness_fear\": (\"nervousness\", \"fear\"),\n",
    "    \"anger_annoyance\": (\"anger\", \"annoyance\"),\n",
    "    \"caring\": (\"caring\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"joy\": (\"joy\", ),\n",
    "    \"neutral\": (\"neutral\", ),\n",
    "    \"optimism\": (\"optimism\", ),\n",
    "    \"desire\": (\"desire\", ),\n",
    "    \"love\": (\"love\", ),\n",
    "    \"disapproval\": (\"disapproval\", ),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"excitement\": (\"excitement\", ),\n",
    "    \"curiosity\": (\"curiosity\", ),\n",
    "    \"embarrassment\": (\"embarrassment\", ),\n",
    "    \"disgust\": (\"disgust\", ),\n",
    "    \"gratitude\": (\"gratitude\", ),\n",
    "    \"confusion\": (\"confusion\", ),\n",
    "    \"disappointment\": (\"disappointment\", ),\n",
    "    \"remorse\": (\"remorse\", )\n",
    "}\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "BOT = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"relief_joy\": (\"relief\", \"joy\"),\n",
    "    \"nervousness\": (\"nervousness\",),\n",
    "    \"fear\": (\"fear\", ),\n",
    "    \"anger\": (\"anger\",),\n",
    "    \"annoyance\": (\"annoyance\",),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"neutral\": (\"neutral\",),\n",
    "    \"optimism\": (\"optimism\",),\n",
    "    \"desire\": (\"desire\",),\n",
    "    \"love\": (\"love\",),\n",
    "    \"disapproval\": (\"disapproval\",),\n",
    "    \"amusement\": (\"amusement\",),\n",
    "    \"caring\": (\"caring\",),\n",
    "    \"excitement\": (\"excitement\",),\n",
    "    \"curiosity\": (\"curiosity\",),\n",
    "    \"embarrassment\": (\"embarrassment\",),\n",
    "    \"disgust\": (\"disgust\",),\n",
    "    \"gratitude\": (\"gratitude\",),\n",
    "    \"confusion\": (\"confusion\",),\n",
    "    \"disappointment\": (\"disappointment\",),\n",
    "    \"remorse\": (\"remorse\",)\n",
    "}\n",
    "\n",
    "BOT_3 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"relief_joy\": (\"relief\", \"joy\"),\n",
    "    \"anger_annoyance\": (\"anger\", \"annoyance\"),\n",
    "    \"nervousness\": (\"nervousness\",),\n",
    "    \"fear\": (\"fear\", ),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"neutral\": (\"neutral\",),\n",
    "    \"optimism\": (\"optimism\",),\n",
    "    \"desire\": (\"desire\",),\n",
    "    \"love\": (\"love\",),\n",
    "    \"disapproval\": (\"disapproval\",),\n",
    "    \"amusement\": (\"amusement\",),\n",
    "    \"caring\": (\"caring\",),\n",
    "    \"excitement\": (\"excitement\",),\n",
    "    \"curiosity\": (\"curiosity\",),\n",
    "    \"embarrassment\": (\"embarrassment\",),\n",
    "    \"disgust\": (\"disgust\",),\n",
    "    \"gratitude\": (\"gratitude\",),\n",
    "    \"confusion\": (\"confusion\",),\n",
    "    \"disappointment\": (\"disappointment\",),\n",
    "    \"remorse\": (\"remorse\",)\n",
    "}\n",
    "\n",
    "BOT_4 = {\n",
    "    \"grief_sadness\": (\"grief\", \"sadness\"),\n",
    "    \"pride_admiration\": (\"pride\", \"admiration\"),\n",
    "    \"relief_joy\": (\"relief\", \"joy\"),\n",
    "    \"anger_annoyance\": (\"anger\", \"annoyance\"),\n",
    "    \"nervousness_fear\": (\"nervousness\", \"fear\"),\n",
    "    \"approval\": (\"approval\", ),\n",
    "    \"realization\": (\"realization\", ),\n",
    "    \"surprise\": (\"surprise\", ),\n",
    "    \"neutral\": (\"neutral\",),\n",
    "    \"optimism\": (\"optimism\",),\n",
    "    \"desire\": (\"desire\",),\n",
    "    \"love\": (\"love\",),\n",
    "    \"disapproval\": (\"disapproval\",),\n",
    "    \"amusement\": (\"amusement\",),\n",
    "    \"caring\": (\"caring\",),\n",
    "    \"excitement\": (\"excitement\",),\n",
    "    \"curiosity\": (\"curiosity\",),\n",
    "    \"embarrassment\": (\"embarrassment\",),\n",
    "    \"disgust\": (\"disgust\",),\n",
    "    \"gratitude\": (\"gratitude\",),\n",
    "    \"confusion\": (\"confusion\",),\n",
    "    \"disappointment\": (\"disappointment\",),\n",
    "    \"remorse\": (\"remorse\",)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e494346e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a3019",
   "metadata": {},
   "source": [
    "## Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9429dee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T02:27:37.300202Z",
     "start_time": "2023-07-31T20:02:44.821256Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\pedro.forli\\AppData\\Local\\Temp\\ipykernel_23360\\1470213174.py:16: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2714/2714 [==============================] - 1266s 462ms/step - loss: 2.1647 - f1_score: 0.4341 - val_loss: 1.8200 - val_f1_score: 0.5347 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "2714/2714 [==============================] - 1267s 467ms/step - loss: 1.6753 - f1_score: 0.5703 - val_loss: 1.8255 - val_f1_score: 0.5512 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "2714/2714 [==============================] - 1283s 473ms/step - loss: 1.4752 - f1_score: 0.6255 - val_loss: 1.8850 - val_f1_score: 0.5576 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "2714/2714 [==============================] - 1237s 456ms/step - loss: 1.4230 - f1_score: 0.6372 - val_loss: 1.8996 - val_f1_score: 0.5543 - lr: 4.0000e-07\n",
      "170/170 [==============================] - 44s 247ms/step\n",
      "170/170 [==============================] - 42s 246ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 53s 477ms/step - loss: 0.2275 - f1_score: 0.6082 - val_loss: 0.2397 - val_f1_score: 0.7212 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 39s 451ms/step - loss: 0.1127 - f1_score: 0.8242 - val_loss: 0.2230 - val_f1_score: 0.7472 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 39s 453ms/step - loss: 0.0786 - f1_score: 0.8881 - val_loss: 0.2360 - val_f1_score: 0.7590 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 39s 452ms/step - loss: 0.0701 - f1_score: 0.9138 - val_loss: 0.2431 - val_f1_score: 0.7346 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 39s 452ms/step - loss: 0.0634 - f1_score: 0.9196 - val_loss: 0.2449 - val_f1_score: 0.7346 - lr: 8.0000e-08\n",
      "170/170 [==============================] - 45s 246ms/step\n",
      "170/170 [==============================] - 42s 247ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "264/264 [==============================] - 134s 463ms/step - loss: 0.1475 - f1_score: 0.5051 - val_loss: 0.1135 - val_f1_score: 0.4924 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 120s 454ms/step - loss: 0.0846 - f1_score: 0.7134 - val_loss: 0.0643 - val_f1_score: 0.8263 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 120s 454ms/step - loss: 0.0532 - f1_score: 0.8809 - val_loss: 0.0716 - val_f1_score: 0.8165 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 120s 454ms/step - loss: 0.0494 - f1_score: 0.8920 - val_loss: 0.0731 - val_f1_score: 0.8165 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 120s 455ms/step - loss: 0.0463 - f1_score: 0.8841 - val_loss: 0.0727 - val_f1_score: 0.8165 - lr: 8.0000e-08\n",
      "170/170 [==============================] - 44s 246ms/step\n",
      "170/170 [==============================] - 42s 246ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 60s 477ms/step - loss: 0.3552 - f1_score: 0.5559 - val_loss: 0.2201 - val_f1_score: 0.6986 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 45s 453ms/step - loss: 0.1950 - f1_score: 0.7683 - val_loss: 0.1825 - val_f1_score: 0.8109 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.1306 - f1_score: 0.8278 - val_loss: 0.2013 - val_f1_score: 0.8215 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.1141 - f1_score: 0.8684 - val_loss: 0.2097 - val_f1_score: 0.8370 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 46s 459ms/step - loss: 0.1152 - f1_score: 0.8547 - val_loss: 0.2099 - val_f1_score: 0.8385 - lr: 8.0000e-08\n",
      "170/170 [==============================] - 45s 252ms/step\n",
      "170/170 [==============================] - 44s 256ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 35s 504ms/step - loss: 0.5234 - f1_score: 0.6961 - val_loss: 0.4326 - val_f1_score: 0.7308 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 21s 454ms/step - loss: 0.3739 - f1_score: 0.8104 - val_loss: 0.4584 - val_f1_score: 0.7436 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 21s 455ms/step - loss: 0.3394 - f1_score: 0.8434 - val_loss: 0.4348 - val_f1_score: 0.7871 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 21s 454ms/step - loss: 0.3160 - f1_score: 0.8495 - val_loss: 0.4360 - val_f1_score: 0.7895 - lr: 4.0000e-07\n",
      "170/170 [==============================] - 44s 245ms/step\n",
      "170/170 [==============================] - 43s 251ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2714/2714 [==============================] - 1512s 553ms/step - loss: 2.1265 - f1_score: 0.4273 - val_loss: 1.7790 - val_f1_score: 0.5446 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "2714/2714 [==============================] - 1403s 517ms/step - loss: 1.6383 - f1_score: 0.5766 - val_loss: 1.7531 - val_f1_score: 0.5583 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "2714/2714 [==============================] - 1265s 466ms/step - loss: 1.4404 - f1_score: 0.6255 - val_loss: 1.8254 - val_f1_score: 0.5627 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "2714/2714 [==============================] - 1235s 455ms/step - loss: 1.3885 - f1_score: 0.6394 - val_loss: 1.8381 - val_f1_score: 0.5619 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "2714/2714 [==============================] - 1234s 455ms/step - loss: 1.3754 - f1_score: 0.6410 - val_loss: 1.8390 - val_f1_score: 0.5615 - lr: 8.0000e-08\n",
      "170/170 [==============================] - 44s 245ms/step\n",
      "170/170 [==============================] - 42s 245ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 54s 478ms/step - loss: 0.3043 - f1_score: 0.5526 - val_loss: 0.2020 - val_f1_score: 0.7935 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 39s 451ms/step - loss: 0.1378 - f1_score: 0.7510 - val_loss: 0.2155 - val_f1_score: 0.7245 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 39s 451ms/step - loss: 0.1015 - f1_score: 0.8373 - val_loss: 0.2184 - val_f1_score: 0.7488 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 39s 451ms/step - loss: 0.0972 - f1_score: 0.8611 - val_loss: 0.2195 - val_f1_score: 0.7488 - lr: 4.0000e-07\n",
      "170/170 [==============================] - 44s 244ms/step\n",
      "170/170 [==============================] - 41s 244ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "264/264 [==============================] - 135s 463ms/step - loss: 0.1222 - f1_score: 0.6019 - val_loss: 0.0500 - val_f1_score: 0.8899 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 127s 480ms/step - loss: 0.0565 - f1_score: 0.8375 - val_loss: 0.0369 - val_f1_score: 0.9283 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 123s 465ms/step - loss: 0.0377 - f1_score: 0.9018 - val_loss: 0.0402 - val_f1_score: 0.9273 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 120s 455ms/step - loss: 0.0356 - f1_score: 0.9135 - val_loss: 0.0385 - val_f1_score: 0.9273 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 120s 456ms/step - loss: 0.0311 - f1_score: 0.9149 - val_loss: 0.0384 - val_f1_score: 0.9273 - lr: 8.0000e-08\n",
      "170/170 [==============================] - 44s 246ms/step\n",
      "170/170 [==============================] - 42s 246ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 59s 477ms/step - loss: 0.3480 - f1_score: 0.5949 - val_loss: 0.2000 - val_f1_score: 0.8212 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.2067 - f1_score: 0.7782 - val_loss: 0.1723 - val_f1_score: 0.8402 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.1431 - f1_score: 0.8419 - val_loss: 0.1513 - val_f1_score: 0.8765 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 45s 454ms/step - loss: 0.1238 - f1_score: 0.8662 - val_loss: 0.1522 - val_f1_score: 0.8765 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.1358 - f1_score: 0.8567 - val_loss: 0.1517 - val_f1_score: 0.8765 - lr: 8.0000e-08\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 45s 455ms/step - loss: 0.1399 - f1_score: 0.8516 - val_loss: 0.1516 - val_f1_score: 0.8765 - lr: 1.6000e-08\n",
      "170/170 [==============================] - 45s 246ms/step\n",
      "170/170 [==============================] - 42s 246ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "236/236 [==============================] - 122s 465ms/step - loss: 0.7184 - f1_score: 0.7090 - val_loss: 0.6864 - val_f1_score: 0.7460 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "236/236 [==============================] - 107s 455ms/step - loss: 0.5179 - f1_score: 0.7957 - val_loss: 0.7089 - val_f1_score: 0.7496 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "236/236 [==============================] - 107s 455ms/step - loss: 0.4309 - f1_score: 0.8384 - val_loss: 0.7966 - val_f1_score: 0.7501 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "236/236 [==============================] - 107s 455ms/step - loss: 0.4145 - f1_score: 0.8512 - val_loss: 0.8094 - val_f1_score: 0.7479 - lr: 4.0000e-07\n",
      "170/170 [==============================] - 44s 246ms/step\n",
      "170/170 [==============================] - 42s 246ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2714/2714 [==============================] - 1323s 483ms/step - loss: 2.1024 - f1_score: 0.4487 - val_loss: 1.7525 - val_f1_score: 0.5495 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "2714/2714 [==============================] - 1325s 488ms/step - loss: 1.6366 - f1_score: 0.5790 - val_loss: 1.7601 - val_f1_score: 0.5647 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "2714/2714 [==============================] - 1316s 485ms/step - loss: 1.4426 - f1_score: 0.6315 - val_loss: 1.8368 - val_f1_score: 0.5661 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "2714/2714 [==============================] - 1305s 481ms/step - loss: 1.3857 - f1_score: 0.6420 - val_loss: 1.8506 - val_f1_score: 0.5642 - lr: 4.0000e-07\n",
      "170/170 [==============================] - 48s 270ms/step\n",
      "170/170 [==============================] - 45s 265ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "87/87 [==============================] - 58s 523ms/step - loss: 0.2368 - f1_score: 0.5696 - val_loss: 0.1985 - val_f1_score: 0.7688 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 41s 477ms/step - loss: 0.1181 - f1_score: 0.8019 - val_loss: 0.2518 - val_f1_score: 0.6963 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 41s 466ms/step - loss: 0.0757 - f1_score: 0.8978 - val_loss: 0.2748 - val_f1_score: 0.6963 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 41s 469ms/step - loss: 0.0698 - f1_score: 0.8872 - val_loss: 0.2801 - val_f1_score: 0.6963 - lr: 4.0000e-07\n",
      "170/170 [==============================] - 46s 259ms/step\n",
      "170/170 [==============================] - 43s 254ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "264/264 [==============================] - 141s 485ms/step - loss: 0.1336 - f1_score: 0.5740 - val_loss: 0.0527 - val_f1_score: 0.8220 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "264/264 [==============================] - 126s 479ms/step - loss: 0.0665 - f1_score: 0.8117 - val_loss: 0.0416 - val_f1_score: 0.8924 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "264/264 [==============================] - 125s 475ms/step - loss: 0.0467 - f1_score: 0.8767 - val_loss: 0.0447 - val_f1_score: 0.8873 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "264/264 [==============================] - 130s 494ms/step - loss: 0.0405 - f1_score: 0.9003 - val_loss: 0.0435 - val_f1_score: 0.9097 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "264/264 [==============================] - 132s 501ms/step - loss: 0.0386 - f1_score: 0.8865 - val_loss: 0.0436 - val_f1_score: 0.9097 - lr: 8.0000e-08\n",
      "170/170 [==============================] - 48s 270ms/step\n",
      "170/170 [==============================] - 44s 260ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 65s 530ms/step - loss: 0.2972 - f1_score: 0.6409 - val_loss: 0.1904 - val_f1_score: 0.7934 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.1815 - f1_score: 0.7949 - val_loss: 0.1905 - val_f1_score: 0.7777 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 49s 486ms/step - loss: 0.1425 - f1_score: 0.8515 - val_loss: 0.1728 - val_f1_score: 0.8250 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.1208 - f1_score: 0.8582 - val_loss: 0.1737 - val_f1_score: 0.8250 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 48s 483ms/step - loss: 0.1263 - f1_score: 0.8635 - val_loss: 0.1737 - val_f1_score: 0.8418 - lr: 8.0000e-08\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 49s 496ms/step - loss: 0.1133 - f1_score: 0.8719 - val_loss: 0.1738 - val_f1_score: 0.8418 - lr: 1.6000e-08\n",
      "170/170 [==============================] - 50s 275ms/step\n",
      "170/170 [==============================] - 48s 285ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "236/236 [==============================] - 130s 494ms/step - loss: 0.8227 - f1_score: 0.6656 - val_loss: 0.7363 - val_f1_score: 0.7057 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "236/236 [==============================] - 112s 473ms/step - loss: 0.7349 - f1_score: 0.6919 - val_loss: 0.6707 - val_f1_score: 0.7355 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "236/236 [==============================] - 112s 475ms/step - loss: 0.6216 - f1_score: 0.7442 - val_loss: 0.6766 - val_f1_score: 0.7443 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "236/236 [==============================] - 114s 481ms/step - loss: 0.5847 - f1_score: 0.7610 - val_loss: 0.6910 - val_f1_score: 0.7426 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "236/236 [==============================] - 113s 479ms/step - loss: 0.5848 - f1_score: 0.7624 - val_loss: 0.6938 - val_f1_score: 0.7446 - lr: 8.0000e-08\n",
      "170/170 [==============================] - 46s 255ms/step\n",
      "170/170 [==============================] - 43s 255ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - 36s 513ms/step - loss: 0.6331 - f1_score: 0.6001 - val_loss: 0.5677 - val_f1_score: 0.4615 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 21s 461ms/step - loss: 0.6181 - f1_score: 0.6034 - val_loss: 0.5105 - val_f1_score: 0.4615 - lr: 1.0000e-05\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 21s 460ms/step - loss: 0.6054 - f1_score: 0.6080 - val_loss: 0.5049 - val_f1_score: 0.4615 - lr: 2.0000e-06\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 21s 467ms/step - loss: 0.5964 - f1_score: 0.6023 - val_loss: 0.5014 - val_f1_score: 0.4615 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 23s 493ms/step - loss: 0.5977 - f1_score: 0.6212 - val_loss: 0.5008 - val_f1_score: 0.4615 - lr: 8.0000e-08\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 22s 481ms/step - loss: 0.5746 - f1_score: 0.6270 - val_loss: 0.5007 - val_f1_score: 0.4615 - lr: 1.6000e-08\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 22s 479ms/step - loss: 0.5892 - f1_score: 0.6142 - val_loss: 0.5007 - val_f1_score: 0.4615 - lr: 3.2000e-09\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 22s 477ms/step - loss: 0.5956 - f1_score: 0.6016 - val_loss: 0.5007 - val_f1_score: 0.4615 - lr: 6.4000e-10\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 22s 473ms/step - loss: 0.5931 - f1_score: 0.5907 - val_loss: 0.5007 - val_f1_score: 0.4615 - lr: 1.2800e-10\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 21s 466ms/step - loss: 0.6142 - f1_score: 0.6108 - val_loss: 0.5007 - val_f1_score: 0.4615 - lr: 2.5600e-11\n",
      "170/170 [==============================] - 47s 260ms/step\n",
      "170/170 [==============================] - 43s 254ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pedro.forli\\.conda\\envs\\w266\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(33)\n",
    "tf.random.set_seed(33)\n",
    "\n",
    "for grid in [\n",
    "     {\n",
    "        \"model\": \"bert-base-uncased\",\n",
    "        \"trainable\": \"all\",\n",
    "        \"head\": \"none\",\n",
    "        \"scheduler\": \"0.2\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"label_smoothing\": 0,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.00005,\n",
    "        \"emoji_tagging\": True,\n",
    "        \"clean_data\": False,\n",
    "        \"balancing\": \"none\",\n",
    "        \"augment\": False,\n",
    "        \"minority_shuffling\": BOT_3,\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"bert-base-uncased\",\n",
    "        \"trainable\": \"all\",\n",
    "        \"head\": \"none\",\n",
    "        \"scheduler\": \"0.2\",\n",
    "        \"dropout\": 0.3,\n",
    "        \"label_smoothing\": 0,\n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.00005,\n",
    "        \"emoji_tagging\": True,\n",
    "        \"clean_data\": False,\n",
    "        \"balancing\": \"none\",\n",
    "        \"augment\": False,\n",
    "        \"minority_shuffling\": BOT_4,\n",
    "    },\n",
    "]:\n",
    "    limit_mem()\n",
    "    tf.keras.backend.clear_session()\n",
    "    while gc.collect():\n",
    "        continue\n",
    "    run_model_experiment(grid, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddd15b4",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
