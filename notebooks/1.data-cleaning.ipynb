{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:41.256036Z",
     "start_time": "2023-07-16T18:39:38.675661Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import contractions\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test data are TSV's without headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:41.333182Z",
     "start_time": "2023-07-16T18:39:41.257041Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.tsv\", sep=\"\\t\", header=None)\n",
    "train.columns = [\"text\", \"emotion\", \"code\"]\n",
    "\n",
    "val = pd.read_csv(\"../data/dev.tsv\", sep=\"\\t\", header=None)\n",
    "val.columns = [\"text\", \"emotion\", \"code\"]\n",
    "\n",
    "test = pd.read_csv(\"../data/test.tsv\", sep=\"\\t\", header=None)\n",
    "test.columns = [\"text\", \"emotion\", \"code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:41.364778Z",
     "start_time": "2023-07-16T18:39:41.334042Z"
    }
   },
   "outputs": [],
   "source": [
    "assert train[\"code\"].nunique() == train.shape[0]\n",
    "assert val[\"code\"].nunique() == val.shape[0]\n",
    "assert test[\"code\"].nunique() == test.shape[0]\n",
    "assert len(set(train[\"code\"]).intersection(test[\"code\"])) == 0\n",
    "assert len(set(train[\"code\"]).intersection(val[\"code\"])) == 0\n",
    "assert len(set(val[\"code\"]).intersection(test[\"code\"])) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the name of the emotions according to the labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:41.380708Z",
     "start_time": "2023-07-16T18:39:41.366640Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../data/emotions.txt\") as r:\n",
    "    emotions = r.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map the GoEmotions data against other classifications as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:41.395872Z",
     "start_time": "2023-07-16T18:39:41.382716Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../data/ekman_mapping.json\") as r:\n",
    "    ekman_map = json.load(r)\n",
    "with open(\"../data/sentiment_mapping.json\") as r:\n",
    "    sentiment_map = json.load(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T14:44:00.258019Z",
     "start_time": "2023-07-15T14:44:00.237019Z"
    }
   },
   "source": [
    "## Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to concatenate train and test data and extract all present emotions within each text, then we are going to map these emotions to ekman's and sentiment classification, and finally add a column with the set to which each sentence belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:41.888895Z",
     "start_time": "2023-07-16T18:39:41.396754Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_map = dict()\n",
    "\n",
    "df = pd.concat([train, val, test])\n",
    "\n",
    "arr = np.array(\n",
    "    df[\"emotion\"].apply(lambda x: [int(v) for v in str(x).split(\",\")])\n",
    "    .apply(lambda x: [int(i in x) for i in range(28)])\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "labels_goemotion = pd.DataFrame(arr, index=df[\"code\"], columns=emotions)\n",
    "complete = (\n",
    "    labels_goemotion.reset_index()\n",
    "    .melt(id_vars=[\"code\"], var_name=\"goemotion\", value_name=\"flag\")\n",
    "    .loc[lambda f: f[\"flag\"] == 1]\n",
    "    .drop(columns=[\"flag\"])\n",
    "    .assign(ekman=lambda f: f[\"goemotion\"].replace({k: e for e, l in ekman_map.items() for k in l}))\n",
    "    .assign(sentiment=lambda f: f[\"goemotion\"].replace({k: e for e, l in sentiment_map.items() for k in l}))\n",
    "    .assign(\n",
    "        set=lambda f: np.where(\n",
    "            f[\"code\"].isin(train.code), \"train\", np.where(f[\"code\"].isin(val.code), \"validation\", \"test\")\n",
    "        )\n",
    "    )\n",
    "    .merge(df.drop(columns=[\"emotion\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to clean text as an alternative approach to the baseline model. In this version, we are going to:\n",
    "1. Replace contractions with the full word\n",
    "2. Replace emojis with tags [EMOJI_...]\n",
    "3. Remove handles such as @something\n",
    "4. Normalize everything to lower case\n",
    "5. Remove http and https links\n",
    "6. Remove unwanted chars such as /, ;, etc.\n",
    "7. Remove stop words\n",
    "8. Remove ponctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:41.904761Z",
     "start_time": "2023-07-16T18:39:41.889753Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def clean_content(text):\n",
    "    # replaces abbreviations with full word versions\n",
    "    clean_text = contractions.fix(text)\n",
    "    \n",
    "    # replaces emojis\n",
    "    clean_text = \"\".join(\n",
    "        [c if c not in emoji.EMOJI_DATA else emoji.EMOJI_DATA[c][\"en\"].replace(\":\", \" _EMOJI_ \") for c in clean_text]\n",
    "    )\n",
    "    \n",
    "    # remove reddit handles\n",
    "    clean_text = re.sub(r\"@\\w+\\s?\", \"\", clean_text)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    clean_text = clean_text.lower()\n",
    "    \n",
    "    # remove links http:// or https://\n",
    "    clean_text = re.sub(r\"https?:\\/\\/\\S+\", \"\", clean_text)\n",
    "    \n",
    "    # remove links beginning with www. and ending with .com\n",
    "    clean_text = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", \"\", clean_text)\n",
    "    \n",
    "    # remove html reference characters\n",
    "    clean_text = re.sub(r\"&[a-z]+;\", \"\", clean_text)\n",
    "    \n",
    "    # remove non-letter characters besides spaces \"/\", \";\" \"[\", \"]\" \"=\", \"#\"\n",
    "    clean_text = re.sub(r\"\\[name\\]\", \"NAME\", clean_text)  \n",
    "    clean_text = re.sub(r\"\\[religion\\]\", \"RELIGION\", clean_text)  \n",
    "    clean_text = re.sub(r\"[/;\\[\\]=#]\", \"\", clean_text)  \n",
    "    clean_text = clean_text.split()\n",
    "    \n",
    "    # remove stop words\n",
    "    clean_lst = []\n",
    "    for word in clean_text:\n",
    "        if word not in stop_words:\n",
    "            clean_lst.append(word)\n",
    "    \n",
    "    # apply lemmatization\n",
    "    lemmatized_words = []\n",
    "    for word in clean_lst:\n",
    "        lemmatized_word = WordNetLemmatizer().lemmatize(word)\n",
    "        lemmatized_words.append(lemmatized_word)\n",
    "    clean_text = \" \".join(lemmatized_words)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_punctuation(word_list):\n",
    "    PUNCUATION_LIST = list(string.punctuation)\n",
    "    return \" \".join([w for w in word_list if w not in PUNCUATION_LIST])\n",
    "\n",
    "\n",
    "def readd_emoji_tags(text):\n",
    "    open_emoji = False\n",
    "    full_sentence = \"\"\n",
    "    for sentence in text.split(\"_emoji_\"):\n",
    "        if open_emoji:\n",
    "            full_sentence += \" [EMOJI_\" + sentence.upper().strip() + \"] \"\n",
    "        else:\n",
    "            full_sentence += sentence\n",
    "        open_emoji = not open_emoji\n",
    "        \n",
    "    full_sentence = re.sub(\" +\", \" \", full_sentence)\n",
    "    return full_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:48.137752Z",
     "start_time": "2023-07-16T18:39:41.905753Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply cleaning process\n",
    "complete[\"clean_text\"] = complete[\"text\"].apply(lambda x : clean_content(x))\n",
    "\n",
    "# # splitting into tokens, features of the structure of the text used in Twitter\n",
    "complete[\"clean_text\"] = complete[\"clean_text\"].apply(TweetTokenizer().tokenize)\n",
    "\n",
    "# remove punctuation marks\n",
    "complete[\"clean_text\"] = complete[\"clean_text\"].apply(remove_punctuation)\n",
    "\n",
    "# clean weird chars\n",
    "complete[\"clean_text\"] = complete[\"clean_text\"].apply(lambda x: \"\".join([w for w in x if ord(w) < 2000]))\n",
    "\n",
    "# re-add name, religion and emoji tags tags\n",
    "complete[\"clean_text\"] = complete[\"clean_text\"].str.replace(\"NAME\", \"[NAME]\").str.replace(\"RELIGION\", \"[RELIGION]\")\n",
    "complete[\"clean_text\"] = complete[\"clean_text\"].apply(readd_emoji_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:48.167753Z",
     "start_time": "2023-07-16T18:39:48.138753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['In an unfunny situation you made me laugh, so thanks 😅',\n",
       "        'unfunny situation made laugh thanks [EMOJI_GRINNING_FACE_WITH_SWEAT]'],\n",
       "       [\"I'm so glad I saw this!!! Now I know how to wash my face! I can't wait for the tooth brushing tutorial🤗\",\n",
       "        'glad saw this know wash face cannot wait tooth brushing tutorial [EMOJI_SMILING_FACE_WITH_OPEN_HANDS]'],\n",
       "       [\"Thank you! I'm going to do my very best. ❤❤❤\",\n",
       "        'thank you going best [EMOJI_RED_HEART] [EMOJI_RED_HEART] [EMOJI_RED_HEART]'],\n",
       "       ['I’m so sorry 🤪', 'sorry [EMOJI_ZANY_FACE]'],\n",
       "       [\"I'm here for you😄😜\",\n",
       "        '[EMOJI_GRINNING_FACE_WITH_SMILING_EYES] [EMOJI_WINKING_FACE_WITH_TONGUE]'],\n",
       "       ['Planning to make a comeback soon™',\n",
       "        'planning make comeback soon [EMOJI_TRADE_MARK]'],\n",
       "       ['Dam 😣 that was awesome!',\n",
       "        'dam [EMOJI_PERSEVERING_FACE] awesome'],\n",
       "       ['They likely just didn’t have enough forex to pay their bill 😂',\n",
       "        'likely enough forex pay bill [EMOJI_FACE_WITH_TEARS_OF_JOY]'],\n",
       "       [\"I'm pretty jelly now, lol. Esp. The pear shape 😧\",\n",
       "        'pretty jelly now lol esp pear shape [EMOJI_ANGUISHED_FACE]'],\n",
       "       [\"Thank you, love. You too! ❤ It's nice to find kindred spirits.\",\n",
       "        'thank you love too [EMOJI_RED_HEART] nice find kindred spirits']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete.loc[lambda f: f[\"clean_text\"].str.contains(\"\\[EMOJI\")].sample(10)[[\"text\", \"clean_text\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:39:48.294782Z",
     "start_time": "2023-07-16T18:39:48.168753Z"
    }
   },
   "outputs": [],
   "source": [
    "complete.to_parquet(\"../data/clean_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
